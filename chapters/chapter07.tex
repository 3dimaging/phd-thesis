\chapter{Further insights from importances}\label{ch:applications}

\begin{remark}{Outline}
In this chapter, we build upon results from Chapter~\ref{ch:importances} to
further study variable importances as computed from random forests. In
Section~\ref{sec:7:redundant}, we first examine importances for variables that
are redundant. In Section~\ref{sec:7:bias}, we revisit variable importances in
the context of binary decision trees and ordered variables. In this framework, we
highlight various sources of bias that may concurrently happen when
importances are computed from actual random forests. Finally, we present in
Section~\ref{sec:7:applications} some succesful applications
of variable importance measures.
\end{remark}


\section{Redundant variables}
\label{sec:7:redundant}

In most machine learning problems, it is typical for input variables to be
correlated, at least to some extent, and to share common bits of information.
In image classification for instance, pixels are usually highly correlated and
individually represent nearly the same information as their neighbors. In that
sense, variables are often \textit{partially redundant}, i.e., some of the
variables may share some of the same information about the ouput variable $Y$.
In the extreme case, redundancy is \textit{total}  or \textit{complete}, with
some of the variables redundantly conveying exactly the same information with
respect to the output variable $Y$. In this section, we study redundancy in
random forests and show that it may have a significant effect on both the
accuracy of the ensemble and variable importance measures.

As a guiding example for our discussion, let us consider a set of input
variables and let us discuss the effect of adding redundant variables on the
structure of randomized trees. Intuitively, two variables $X_i$ and $X_j$ are
redundant if one can perfectly explains the other and vice-versa. Formally, we
define redundancy as follows:

\begin{definition}
Two variables $X_i, X_j$ are totally redundant if the mutual information
$I(X_i;X_j)$ is maximum.
\end{definition}

In particular, a variable $X_j$ and its copy $X_j^\prime$ are totally
redundant. With respect to random forests, adding copies of variables (e.g.,
duplicating $X_j$, hence resulting in a new set of $p+1$ input variables) has
no effect when the selection of the split is deterministic (e.g., in RF for $K$
set to the maximum value). No matter the number of totally redundant variables,
the best split that is selected is always the same, even if the same splits
need to be recomputed multiple times due to redundancy. When the choice of the
best split is stochastic however (e.g., for $K$ strictly smaller than the total number
of variables), adding multiple copies of a variable $X_j$ results in
splits that may be biased towards this variable (or one of its copies), which
in turn may have a significant effect on the resulting accuracy of the
ensemble. For a fixed value of $K$, it is indeed not difficult to see that
adding copies of $X_j$ increases the probability of $X_j$, or of one of its
copies, to be in the random subset of $K$ input variables on which to look for
splits. As a corollary, it therefore also simultaneously decreases the
probability of any of the others to be selected, hence biasing
the structure of resulting decision trees. Note that the resulting net effect
on accuracy depends on the nature of duplicated variable. If $X_j$ is very
informative with respect to the input, then favouring splits on $X_j$ by
adding copies may result in an increase of accuracy. By contrast, if $X_j$
is irrelevant, then adding  copies increases the risk of overfitting.

With respect to variable importances, the effect of adding redundant variables
can be derived both qualitatively and quantitavely using results from
Chapter~\ref{ch:importances}. From Theorem~\ref{thm:relevant}, we already know
that adding irrelevant variables does not change the resulting variable
importances. Adding copies of a relevant variable however, has an effect on
both the importance of the duplicated variable and on the importance of the
remaining variables. As in the previous chapter, let us assume a set $V= \{X_1,
..., X_p\}$  of categorical input variables and a categorical output $Y$, for
which we derive the MDI importance, as computed from totally randomized and
fully developed trees built on an infinitely large dataset.

\begin{lemma}\label{lemma:red1}
Let $X_i$ and $X_j$ be totally redundant variables. For any conditioning set
$B$,
\begin{align}
& I(X_i;Y|B,X_j) = I(X_j;Y|B, X_i) = 0 \label{lemma:red1:eqn1} \\
& I(X_i;Y|B) = I(X_j;Y|B) \label{lemma:red1:eqn2}.
\end{align}
\end{lemma}

\begin{proof}
By definition, if $X_i$ and $X_j$ are totally redundant, then
\begin{equation}
I(X_i;X_j) = H(X_i) - H(X_i|X_j)
\end{equation}
is maximum, which happens when $H(X_i|X_j) = 0$ since $H$ is non-negative.
By symmetry of the mutual information, we similarly have $H(X_j|X_i)=0$,
and therefore $H(X_i)=H(X_j)$. Using the same argument, this directly extends
to any conditioning set $B$, giving:
\begin{align}
& H(X_i|B,X_j) = 0\\
& H(X_j|B,X_i) = 0\\
& H(X_i|B) = H(X_j|B)
\end{align}

From these, it follows that,
\begin{align}
& I(X_i;Y|B,X_j) = H(X_i|B, X_j) - H(X_i|B,X_j,Y) = 0 - 0, \\
& I(X_j;Y|B,X_i) = H(X_j|B, X_i) - H(X_j|B,X_i,Y) = 0 - 0,
\end{align}
which proves Equation~\ref{lemma:red1:eqn1}. We also have
\begin{align}
I(X_i;Y|B) &= H(X_i|B) - H(X_i|B,Y) \\
           &= H(X_j|B) - H(X_j|B,Y) \\
           &= I(X_j;Y|B),
\end{align}
which proves Equation~\ref{lemma:red1:eqn2}.
\end{proof}

\begin{proposition}\label{prop:red:self}
Let $X_j\in V$ be relevant variable with respect to $Y$ and $V$ and let
$X_j^\prime \notin V$ be a totally redundant variable with respect to $X_j$.
The infinite sample size importance of $X_j$ as computed with an infinite
ensemble of fully developed totally randomized trees built on $V\cup
\{X_j^\prime\}$ is
\begin{equation}
\text{Imp}(X_j) = \sum_{k=0}^{p-1} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{equation}
\end{proposition}

\begin{proof}
From Theorem~\ref{thm:imp}, the variable importance of $X_j$ is
\begin{align}
\text{Imp}(X_j) &= \sum_{k=0}^{p-1+1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j} \cup \{X_j^\prime\})} I(X_j;Y|B) \nonumber \\
&= \sum_{k=0}^{p-1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B) \nonumber \\
&= \sum_{k=0}^{p-1} \frac{p-k}{p+1} \frac{1}{C_{p}^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B),
\end{align}
since from Lemma~\ref{lemma:red1}, $I(X_j;Y|B\cup X_j^\prime)=0$ for all $B \in {\cal P}(V^{-j})$.
\end{proof}

\begin{lemma}\label{lemma:red2}
Let $X_i$ and $X_j$ be totally redundant variables. For any conditioning set
$B$ and for any variable $X_l$,
\begin{equation}
I(X_l;Y|B,X_i) = I(X_l;Y|B, X_j) = I(X_l;Y|B, X_i, X_j) \label{lemma:red2:eqn}
\end{equation}
\end{lemma}

\begin{proof}
From the chaining rule of the mutual information, we have
\begin{align}
I(X_i,X_j,X_l;Y|B) &= I(X_l;Y|B) + I(X_i,X_j;Y|B,X_l) \nonumber \\
                   &= I(X_l;Y|B) + I(X_i;Y|B,X_l) + I(X_i;Y|B, X_j, X_l) \nonumber \\
                   &= I(X_l;Y|B) + I(X_i;Y|B,X_l) \quad\text{(Lemma~\ref{lemma:red1})} \nonumber \\
                   &= I(X_i, X_l;Y|B) \nonumber \\
                   &= I(X_i;Y|B) + I(X_l;Y|B,X_i) \label{lemma:red2:eqn1}.
\end{align}
By symmetry,
\begin{equation}\label{lemma:red2:eqn2}
I(X_i,X_j,X_l;Y|B) = I(X_j;Y|B) + I(X_l;Y|B,X_j),
\end{equation}
which proves that $I(X_l;Y|B,X_i) = I(X_l;Y|B, X_j)$, by combining both equations
and using the fact that $I(X_i;Y|B) = I(X_j;Y|B)$ (Lemma~\ref{lemma:red1}).

From the chaining rule, we also have
\begin{align}
I(X_i,X_j,X_l;Y|B) &= I(X_i, X_j;Y|B) + I(X_l;Y|B,X_i,X_j) \nonumber \\
                   &= I(X_i; Y|B) + I(X_j;Y|B, X_i) + I(X_l;Y|B,X_i,X_j) \nonumber \\
                   &= I(X_i; Y|B) + I(X_l;Y|B,X_i,X_j).
\end{align}
By combining this last equation with Equation~\ref{lemma:red2:eqn1},
we finally have $I(X_l;Y|B,X_i) = I(X_l;Y|B,X_i,X_j)$, which proves
Lemma~\ref{lemma:red2}.
\end{proof}

\begin{proposition}\label{prop:red:other}
Let $X_j\in V$ be relevant variable with respect to $Y$ and $V$ and let
$X_j^\prime \notin V$ be a totally redundant variable with respect to $X_j$.
The infinite sample size importance of $X_l \in V$ as computed with an infinite
ensemble of fully developed totally randomized trees built on $V\cup
\{X_j^\prime\}$ is
\begin{align}
\text{Imp}(X_l) &= \sum_{k=0}^{p-2} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \\
                & \hookrightarrow \sum_{k=0}^{p-2}  \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right]  \sum_{B \in {\cal P}_k(V^{-l}\setminus X_j)} I(X_l;Y|B\cup X_j). \nonumber
\end{align}
\end{proposition}

\begin{proof}
From Lemma~\ref{lemma:red2}, conditioning by either $X_j$, $X_j^\prime$ or by
both  variables yield terms $I(X_l;Y|B,X_j)$, $I(X_l;Y|B,X_j^\prime)$ and
$I(X_l;Y|B,X_j,X_j^\prime)$ that are all equal. From Theorem~\ref{thm:imp}, the
variable importance of $X_l$ can therefore be rewritten as follows:

\begin{align}
\text{Imp}(X_l) &= \sum_{k=0}^{p-1+1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-l}\cup X_j^\prime)} I(X_l;Y|B) \nonumber \\
           &= \sum_{k=0}^{p-2} \sum_{k'=0}^2 \frac{1}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \sum_{\substack{B \in {\cal P}_k(V^{-l} \setminus X_j)\\ B' \in {\cal P}_{k'}(\{X_j, X_j^\prime\})} } I(X_l;Y|B \cup B') \nonumber \\
           &= \sum_{k=0}^{p-2} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \nonumber \\
           & \hookrightarrow \sum_{k=0}^{p-2} \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right] \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B \cup X_j) \nonumber \\
           &= \sum_{k=0}^{p-2} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \nonumber \\
           & \hookrightarrow \sum_{k=0}^{p-2}  \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right]  \sum_{B \in {\cal P}_k(V^{-l}\setminus X_j)} I(X_l;Y|B\cup X_j). \nonumber
\end{align}
\end{proof}

Proposition~\ref{prop:red:self} shows that the importance of $X_j$ decreases
when the input variables are expanded with a redundant variable. Intuitively,
this result is in fact expected since the same information is then conveyed
within two variables. It also shows that the terms in the total importance are
not all reweighted in the same way. The weight of the terms corresponding to
small conditioning sets remains nearly unchanged (i.e., for small values of
$k$, $\tfrac{p-k}{p+1}$ is close to $1$), while the weight of the terms of
large conditioning sets is greatly impacted (i.e., for large values of $k$,
$\tfrac{p-k}{p+1}$ tends to $0$)


% interpretation
% extension à C copies => conclusions qualitatives similaires

% conclusion de l'analyse : faire gaffe à l'inteprétation des importances!
%                           des variables peu importantes peuvent l'être
%                           parce qu'elles contiennent une info qui est redondante


\section{Bias in variable importances}
\label{sec:7:bias}

% Splitting with binary trees on non-binary variables is like having
% binary variables and adjust the probability of selecting them
% => étudier les propriétés de ce truc là => biais

% understanding bias
% - when k>1,
%       * masking effects
%       * high cardinality variables are more likely to be selected, even when both equally uninformative
% - but, even for k=1, the importance is biased?
%   => because it is not the same conditionings that are evaluated *** (a branch may comprise several values for b (i.e. B<=b plutot que B=b))
%   => bias may be positive or negative!!
% - masking effects again (stronger in RF than in ETs because not all thresholds are evaluted)
%   c'est commme si on on n'utilisait jamais certaines variables de l'encodage binaire
% - est-ce que la non-equiprobabilité des arbres est une source de biais?
%   => étudier la fréquence des conditionnements pour s'en assurer


% => ID3-like are not biased in that sense
%
% reproduire experience de Strobl
% - which one is biased?


\section{Applications}
\label{sec:7:applications}

% Explain Vincent paper
% reproduire experience de microarray dans "Manual on setting up, using, and understanding random forests v3. 1/v4"
