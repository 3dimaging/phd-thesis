\chapter{Further insights from importances}\label{ch:applications}

\section{Redundant variables}




\section{Importances in binary decision trees}

% Splitting with binary trees on non-binary variables is like having
% binary variables and adjust the probability of selecting them
% => étudier les propriétés de ce truc là => biais

% understanding bias
% - when k>1,
%       * masking effects
%       * high cardinality variables are more likely to be selected, even when both equally uninformative
% - but, even for k=1, the importance is biased?
%   => because it is not the same conditionings that are evaluated *** (a branch may comprise several values for b (i.e. B<=b plutot que B=b))
%   => bias may be positive or negative!!
% - masking effects again (stronger in RF than in ETs because not all thresholds are evaluted)
%   c'est commme si on on n'utilisait jamais certaines variables de l'encodage binaire
% - est-ce que la non-equiprobabilité des arbres est une source de biais?
%   => étudier la fréquence des conditionnements pour s'en assurer


% => ID3-like are not biased in that sense
%
% reproduire experience de Strobl
% - which one is biased?


\section{Applications}

% Explain Vincent paper
% reproduire experience de microarray dans "Manual on setting up, using, and understanding random forests v3. 1/v4"
