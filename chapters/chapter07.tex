\chapter{Further insights from importances}\label{ch:applications}

\begin{remark}{Outline}
In this chapter, we build upon results from Chapter~\ref{ch:importances} to
further study variable importances as computed from random forests. In
Section~\ref{sec:7:redundant}, we first examine importances for variables that
are redundant. In Section~\ref{sec:7:bias}, we revisit variable importances in
the context of binary decision trees and ordered variables. In this framework, we
highlight various sources of bias that may concurrently happen when
importances are computed from actual random forests. Finally, we present in
Section~\ref{sec:7:applications} some successful applications
of variable importance measures.
\end{remark}


\section{Redundant variables}
\label{sec:7:redundant}

In most machine learning problems, it is typical for input variables to be
correlated, at least to some extent, and to share common bits of information.
In image classification for instance, pixels are usually highly correlated and
individually represent nearly the same information as their neighbors. In that
sense, variables are often \textit{partially redundant}, i.e., some of the
variables may share some of the same information about the output variable $Y$.
In the extreme case, redundancy is \textit{total}  or \textit{complete}, with
some of the variables redundantly conveying exactly the same information with
respect to the output variable $Y$. In this section, we study redundancy in
random forests and show that it may have a significant effect on both the
accuracy of the ensemble and variable importance measures.

As a guiding example for our discussion, let us consider a set of input
variables and let us discuss the effect of adding redundant variables on the
structure of randomized trees. Intuitively, two variables $X_i$ and $X_j$ are
redundant if one can perfectly explains the other and vice-versa. Formally, we
define redundancy as follows:

\begin{definition}
Two variables $X_i, X_j$ are totally redundant if the mutual information
$I(X_i;X_j)$ is maximum.
\end{definition}

In particular, a variable $X_j$ and its copy $X_j^\prime$ are totally
redundant. With respect to random forests, adding copies of variables (e.g.,
duplicating $X_j$, hence resulting in a new set of $p+1$ input variables) has
no effect when the selection of the split is deterministic (e.g., in RF for $K$
set to the maximum value). No matter the number of totally redundant variables,
the best split that is selected is always the same, even if the same splits
need to be recomputed multiple times due to redundancy. When the choice of the
best split is stochastic however (e.g., for $K$ strictly smaller than the total number
of variables), adding multiple copies of a variable $X_j$ results in
splits that may be biased towards this variable (or one of its copies), which
in turn may have a significant effect on the resulting accuracy of the
ensemble. For a fixed value of $K$, it is indeed not difficult to see that
adding copies of $X_j$ increases the probability of $X_j$, or of one of its
copies, to be in the random subset of $K$ input variables on which to look for
splits. As a corollary, it therefore also simultaneously decreases the
probability of any of the others to be selected, hence biasing
the structure of resulting decision trees. Note that the resulting net effect
on accuracy depends on the nature of duplicated variable. If $X_j$ is very
informative with respect to the input, then favoring splits on $X_j$ by
adding copies may result in an increase of accuracy. By contrast, if $X_j$
is irrelevant, then adding  copies increases the risk of overfitting.

With respect to variable importances, the effect of adding redundant variables
can be derived both qualitatively and quantitatively using results from
Chapter~\ref{ch:importances}. From Theorem~\ref{thm:relevant}, we already know
that adding irrelevant variables does not change the resulting variable
importances. Adding copies of a relevant variable however, has an effect on
both the importance of the duplicated variable and on the importance of the
remaining variables. As in the previous chapter, let us assume a set $V= \{X_1,
..., X_p\}$  of categorical input variables and a categorical output $Y$, for
which we derive the MDI importance, as computed from totally randomized and
fully developed trees built on an infinitely large dataset.

\begin{lemma}\label{lemma:red1}
Let $X_i$ and $X_j$ be totally redundant variables. For any conditioning set
$B$,
\begin{align}
& I(X_i;Y|B,X_j) = I(X_j;Y|B, X_i) = 0 \label{lemma:red1:eqn1} \\
& I(X_i;Y|B) = I(X_j;Y|B) \label{lemma:red1:eqn2}.
\end{align}
\end{lemma}

\begin{proof}
By definition, if $X_i$ and $X_j$ are totally redundant, then
\begin{equation}
I(X_i;X_j) = H(X_i) - H(X_i|X_j)
\end{equation}
is maximum, which happens when $H(X_i|X_j) = 0$ since $H$ is non-negative.
By symmetry of the mutual information, we similarly have $H(X_j|X_i)=0$,
and therefore $H(X_i)=H(X_j)$. Using the same argument, this directly extends
to any conditioning set $B$, giving:
\begin{align}
& H(X_i|B,X_j) = 0\\
& H(X_j|B,X_i) = 0\\
& H(X_i|B) = H(X_j|B)
\end{align}

From these, it follows that,
\begin{align}
& I(X_i;Y|B,X_j) = H(X_i|B, X_j) - H(X_i|B,X_j,Y) = 0 - 0, \\
& I(X_j;Y|B,X_i) = H(X_j|B, X_i) - H(X_j|B,X_i,Y) = 0 - 0,
\end{align}
which proves Equation~\ref{lemma:red1:eqn1}. We also have
\begin{align}
I(X_i;Y|B) &= H(X_i|B) - H(X_i|B,Y) \\
           &= H(X_j|B) - H(X_j|B,Y) \\
           &= I(X_j;Y|B),
\end{align}
which proves Equation~\ref{lemma:red1:eqn2}.
\end{proof}

\begin{proposition}\label{prop:red:self}
Let $X_j\in V$ be a relevant variable with respect to $Y$ and $V$ and let
$X_j^\prime \notin V$ be a totally redundant variable with respect to $X_j$.
The infinite sample size importance of $X_j$ as computed with an infinite
ensemble of fully developed totally randomized trees built on $V\cup
\{X_j^\prime\}$ is
\begin{equation}
\text{Imp}(X_j) = \sum_{k=0}^{p-1} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{equation}
\end{proposition}

\begin{proof}
From Theorem~\ref{thm:imp}, the variable importance of $X_j$ is
\begin{align}
\text{Imp}(X_j) &= \sum_{k=0}^{p-1+1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j} \cup \{X_j^\prime\})} I(X_j;Y|B) \nonumber \\
&= \sum_{k=0}^{p-1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B) \nonumber \\
&= \sum_{k=0}^{p-1} \frac{p-k}{p+1} \frac{1}{C_{p}^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B),
\end{align}
since from Lemma~\ref{lemma:red1}, $I(X_j;Y|B\cup X_j^\prime)=0$ for all $B \in {\cal P}(V^{-j})$.
\end{proof}

\begin{lemma}\label{lemma:red2}
Let $X_i$ and $X_j$ be totally redundant variables. For any conditioning set
$B$ and for any variable $X_l$,
\begin{equation}
I(X_l;Y|B,X_i) = I(X_l;Y|B, X_j) = I(X_l;Y|B, X_i, X_j) \label{lemma:red2:eqn}
\end{equation}
\end{lemma}

\begin{proof}
From the chaining rule of the mutual information, we have
\begin{align}
I(X_i,X_j,X_l;Y|B) &= I(X_l;Y|B) + I(X_i,X_j;Y|B,X_l) \nonumber \\
                   &= I(X_l;Y|B) + I(X_i;Y|B,X_l) + I(X_i;Y|B, X_j, X_l) \nonumber \\
                   &= I(X_l;Y|B) + I(X_i;Y|B,X_l) \quad\text{(Lemma~\ref{lemma:red1})} \nonumber \\
                   &= I(X_i, X_l;Y|B) \nonumber \\
                   &= I(X_i;Y|B) + I(X_l;Y|B,X_i) \label{lemma:red2:eqn1}.
\end{align}
By symmetry,
\begin{equation}\label{lemma:red2:eqn2}
I(X_i,X_j,X_l;Y|B) = I(X_j;Y|B) + I(X_l;Y|B,X_j),
\end{equation}
which proves that $I(X_l;Y|B,X_i) = I(X_l;Y|B, X_j)$, by combining both equations
and using the fact that $I(X_i;Y|B) = I(X_j;Y|B)$ (Lemma~\ref{lemma:red1}).

From the chaining rule, we also have
\begin{align}
I(X_i,X_j,X_l;Y|B) &= I(X_i, X_j;Y|B) + I(X_l;Y|B,X_i,X_j) \nonumber \\
                   &= I(X_i; Y|B) + I(X_j;Y|B, X_i) + I(X_l;Y|B,X_i,X_j) \nonumber \\
                   &= I(X_i; Y|B) + I(X_l;Y|B,X_i,X_j).
\end{align}
By combining this last equation with Equation~\ref{lemma:red2:eqn1},
we finally have $I(X_l;Y|B,X_i) = I(X_l;Y|B,X_i,X_j)$, which proves
Lemma~\ref{lemma:red2}.
\end{proof}

\begin{proposition}\label{prop:red:other}
Let $X_j\in V$ be a relevant variable with respect to $Y$ and $V$ and let
$X_j^\prime \notin V$ be a totally redundant variable with respect to $X_j$.
The infinite sample size importance of $X_l \in V$ as computed with an infinite
ensemble of fully developed totally randomized trees built on $V\cup
\{X_j^\prime\}$ is
\begin{align}\label{prop:red:other:eqn}
\text{Imp}(X_l) &= \sum_{k=0}^{p-2} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \\
                & \hookrightarrow \sum_{k=0}^{p-2}  \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right]  \sum_{B \in {\cal P}_k(V^{-l}\setminus X_j)} I(X_l;Y|B\cup X_j). \nonumber
\end{align}
\end{proposition}

\begin{proof}
From Lemma~\ref{lemma:red2}, conditioning by either $X_j$, $X_j^\prime$ or by
both  variables yield terms $I(X_l;Y|B,X_j)$, $I(X_l;Y|B,X_j^\prime)$ and
$I(X_l;Y|B,X_j,X_j^\prime)$ that are all equal. From Theorem~\ref{thm:imp}, the
variable importance of $X_l$ can therefore be rewritten as follows:

\begin{align}
\text{Imp}(X_l) &= \sum_{k=0}^{p-1+1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-l}\cup X_j^\prime)} I(X_l;Y|B) \nonumber \\
           &= \sum_{k=0}^{p-2} \sum_{k'=0}^2 \frac{1}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \sum_{\substack{B \in {\cal P}_k(V^{-l} \setminus X_j)\\ B' \in {\cal P}_{k'}(\{X_j, X_j^\prime\})} } I(X_l;Y|B \cup B') \nonumber \\
           &= \sum_{k=0}^{p-2} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \nonumber \\
           & \hookrightarrow \sum_{k=0}^{p-2} \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right] \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B \cup X_j) \nonumber \\
           &= \sum_{k=0}^{p-2} \frac{p-k}{p+1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \nonumber \\
           & \hookrightarrow \sum_{k=0}^{p-2}  \left[ \sum_{k'=1}^2 \frac{C^{k'}_2}{C_{p+1}^{k+k'}} \frac{1}{p+1-(k+k')} \right]  \sum_{B \in {\cal P}_k(V^{-l}\setminus X_j)} I(X_l;Y|B\cup X_j). \nonumber
\end{align}
\end{proof}

Proposition~\ref{prop:red:self} shows that the importance of $X_j$ decreases
when a redundant variable $X_j^\prime$ is added to the set of input variables.
Intuitively, this result is in fact expected since the same information is then
conveyed within two variables (i.e., in $X_j$ and its copy $X_j^\prime$). It
also shows that the terms in the total importance are not all modified in the
same way. The weight of the terms corresponding to small conditioning sets
remains nearly unchanged (i.e., for small values of $k$, $\tfrac{p-k}{p+1}$ is
close to $1$), while the weight of the terms of large conditioning sets is
greatly impacted (i.e., for large values of $k$, $\tfrac{p-k}{p+1}$ tends to
$0$).

As shown by Proposition~\ref{prop:red:other}, the effect of adding a redundant
variable $X_j^\prime$ on the importance of the other variables $X_l$ (for
$l\neq j$) is twofold. The first part of Equation~\ref{prop:red:other:eqn}
shows that the weight of all terms that do not include $X_j$ (or its copy)
strictly decreases by a factor $\tfrac{p-k}{p+1}$. The second part of
Equation~\ref{prop:red:other:eqn} shows that the the weight of all terms that
include $X_j$ (or its copy) increases, since several equivalent conditioning
sets (i.e., $B\cup \{X_j\}$, $B\cup \{X_j\}^\prime$ and $B\cup \{X_j,
X_j^\prime\}$) can now appear within the branches of a tree. Like for
Proposition~\ref{prop:red:self}, impurity terms are not all modified in the
same way and changes depend on the size $k$ of the conditioning set $B$.
Overall, the net effect on the total importance of $X_l$ is therefore a
compromise between these opposing changes. If the weighted sum of the
$I(X_l;Y|B,X_j)$ terms is low with respect to the sum of the terms that do not
include $X_j$ (or its copy), then the decrease effect dominates and the
importance of $X_l$ should be smaller. By contrast, if the $I(X_l;Y|B,X_j)$
terms are larger, then the increase effect dominates and the resulting
importance is larger.

Propositions~\ref{prop:red:self} and \ref{prop:red:other} can be extended to
the case where $N_c$ redundant variables $X_j^c$ (for $c=1,\dots,N_c$) are
added to the input variables instead of one, leading to the general
Proposition~\ref{prop:red:general}. From this result, the same qualitative
conclusions can drawn, except that the decrease or increase effects discussed
above are even stronger as more redundant variables are added.

\begin{proposition}\label{prop:red:general}
Let $X_j\in V$ be a relevant variable with respect to $Y$ and $V$ and let
$X_j^c \notin V$ (for $c=1,\dots,N_c$) be $N_c$ totally redundant variables with respect to $X_j$.
The infinite sample size importances of $X_j$ and $X_l \in V$ as computed with an infinite
ensemble of fully developed totally randomized trees built on $V\cup
\{X_j^1,\dots,X_j^{N_c}\}$ are
\begin{align*}
\text{Imp}(X_j)  &= \sum_{k=0}^{p-1} \left[ \frac{C^k_p (p-k)}{C^k_{p+N_c}(p+N_c-k)}  \right] \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B), \\
\text{Imp}(X_l) &= \sum_{k=0}^{p-2} \left[ \frac{C^k_p (p-k)}{C^k_{p+N_c}(p+N_c-k)}  \right] \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-l} \setminus X_j)} I(X_l;Y|B) + \\
                & \hookrightarrow \sum_{k=0}^{p-2}  \left[ \sum_{k'=1}^{N_c+1} \frac{C^{k'}_{N_c+1}}{C_{p+N_c}^{k+k'}} \frac{1}{p+N_c-(k+k')} \right]  \sum_{B \in {\cal P}_k(V^{-l}\setminus X_j)} I(X_l;Y|B\cup X_j).
\end{align*}
\end{proposition}

\begin{proof}
Omitted here, but Proposition~\ref{prop:red:general} can be proved
by generalizing for $N_c$ the proofs of Propositions~\ref{prop:red:self}
and \ref{prop:red:other}.
\end{proof}

As an illustrative example, let us reconsider the LED classification problem
from Section~\ref{sec:6:illustration}, and for which $X_5$ was found to be the
most important variable. As shown in Figure~\ref{fig:7:red:led}, adding
variables that are redundant with $X_5$ makes its importance decrease, as
predicted by our theoretical result from Propositions~\ref{prop:red:self} and
\ref{prop:red:general}. When 5 or more copies of $X_5$ are added, the
importance of $X_5$ is the smallest of all, as if $X_5$ had become less
informative. Similarly, we observe that the importance of the other variables
remain about the same or slightly decrease, as if they all had become a bit
less informative. With regards to our previous results in Propositions
\ref{prop:red:other} and \ref{prop:red:general}, this indicates that the
importance due to the $I(X_l;Y|B)$ terms prevails from  the importance due to
the $I(X_l;Y|B,X_5^c)$ terms. As a matter of fact, this example highlights a
fundamental property of variable importances as computed in a random forest:
importance measures are computed not only with respect to the output $Y$, but
also with respect to all the other input variables that define the problem. In
particular, a variable which is not important is not necessarily uninformative,
as the example illustrates. A variable may be considered as less important
because the  information it conveys is also redundantly conveyed in other
variables, and not necessarily because it has no information.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/ch7_red_led.pdf}
\caption{Adding copies of $X_5$ on the LED classification task. The more
         redundant variables are added, the lesser the importance of $X_5$.}
\label{fig:7:red:led}
\end{figure}

As a second example, Figure~\ref{fig:7:red:xor} illustrates redundancy effects
for a XOR classification problem defined on two variables $X_1$ and $X_2$.
Again, the importance of the duplicated variable $X_1$  decreases as redundant
variables are added, which confirms our results from
Propositions~\ref{prop:red:self} and \ref{prop:red:general}. More
interestingly, we now observe that the importance of the other variable, $X_2$,
increases as copies of $X_1$ are added. For this problem, the
$I(X_2;Y|B,X_1^c)$ terms are prevalent with respect to the $I(X_2;Y|B)$ terms
(which is in fact unique and equal to $0$), thereby artificially increasing the
overall importance of $X_2$ as redundancy augments, as expected from
Propositions \ref{prop:red:other} and \ref{prop:red:general}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/ch7_red_xor.pdf}
\caption{Adding copies of $X_1$ on a XOR classification task. The more redundant
         variables are added, the lesser the importance of $X_1$, but the larger
         the importance of $X_2$.}
\label{fig:7:red:xor}
\end{figure}

Overall, results presented in this section call for caution when interpreting
variable importance scores. Due to redundancy effects, it may happen that the
total importance of a given variable is either misleadingly low or deceptively
high because the same information is spread within several redundant variables
and therefore taken into account several times within the total importances. As
such, we advise to complement the interpretation with a systematic
decomposition of variable importance scores, e.g., as previously done in
Figure~\ref{fig:decomposition}, in order to better understand why a variable is
in fact important and to possibly detect redundancy.


\section{Bias in variable importances}
\label{sec:7:bias}

% décrire strobl => high cardinalité implique high importance POUR UNE VAR IRRELEVAnt
% commencer par un exemple qui montre que ça tient pas la route
%   - pq ce serait le cas pour K=1? (autant pour RFs et que pour ETs)

% Splitting with binary trees on non-binary variables is like having
% binary variables and adjust the probability of selecting them
% => étudier les propriétés de ce truc là => biais

% understanding bias
% - when k>1,
%       * masking effects
%       * high cardinality variables are more likely to be selected, even when both equally uninformative
%           => true for RF, not for ETs?
%           => if two variable are equally relevant with respect to I(X;Y|B), then cardinality does not matter
%           => however, if they are irrelevant, the larger one is expected to be selected because there is more chance memorize data
%              l'effet s'atténue à mesure que N augmente
% - but, even for k=1, the importance is biased?
%   => because it is not the same conditionings that are evaluated *** (a branch may comprise several values for b (i.e. B<=b plutot que B=b))
%   => bias may be positive or negative!!
% - masking effects again (stronger in RF than in ETs because not all thresholds are evaluted)
%   c'est commme si on on n'utilisait jamais certaines variables de l'encodage binaire
% - est-ce que la non-equiprobabilité des arbres est une source de biais?
%   => étudier la fréquence des conditionnements pour s'en assurer
% - N is finite => we do not consider all splits
%               => I(X;Y) is estimated

% strobl: les variables irrelevant de grande cardinalité sont plus importantes
% => normal dans le cas fini => on peut mémoriser le dataset avec une variable
% continue pour un LS fini


% piste de recherche: approximer splits exhaustifs par des splits limités à n branches max
% bonus: pour n=2, on retrouverait le cas binaire
% quelle est l'erreur d'approximer?
% comment former les paquets à mettre dans chaque branche?

% => ID3-like are not biased in that sense
%
% reproduire experience de Strobl
% - which one is biased?


\section{Applications}
\label{sec:7:applications}

% Explain Vincent paper
% reproduire experience de microarray dans "Manual on setting up, using, and understanding random forests v3. 1/v4"
