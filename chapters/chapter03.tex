\chapter{Classification and regression trees}\label{ch:cart}

\begin{remark}{Outline}
In this chapter, we present a unified framework in which we detail
(single) decision trees methods. In Section~\ref{sec:3:introduction}, we
first give an overview of the context in which these algorithms have been developed.
In Section~\ref{sec:3:tree-structured-models}, we proceed with a mathematical
presentation, introducing all necessary concepts and notations. The general learning
algorithm is then presented in Section~\ref{sec:3:induction} and discussed
in finer details in Sections~\ref{sec:3:splitting-rules} and \ref{sec:3:criteria}.
\end{remark}

\section{Introduction}
\label{sec:3:introduction}

Since always, artificial intelligence has been driven by the ambition to
understand and uncover complex relations in data. That is, to find models that
can not only produce accurate predictions, but also be used to extract
knowledge in an intelligible way. Guided with this twofold objective, research
in machine learning has given rise to extensive bodies works in a myriad of
directions. Among all of them however, \textit{classification and regression
trees} stand as one of the most effective and useful method, capable to
produce both reliable and understandable results, on mostly any kind of data.



% * Citer les premiers travaux
%       Historique
%       Présentation unifiée de ID3, C4.5 et CART
%       foundation to many modern data mining approaches based on bagging and boosting.
% * Review Zighed,\citep{criminisi:2013}
% * Avantages (cart 56+)
%       - can handle any type of data (both ordered and categorical)
%       - handle conditional information in non-homogeneous relations => consistence
%       - intepretability (cart 58, non-statistical interpretation)
%       - robust to noisy features <=> intrinstic feature selection
%       - invariant with monotone transforme of variables
%       - robust to outliers and misclassification

\section{Tree structured models}
\label{sec:3:tree-structured-models}

% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4, Zighed
%     > définitions formelles
%     > Data structures (arrays vs. object-oriented structures)
%     > Intuitively, when all variables are ordered, it is like cutting X into rectangles (see CART 31+)
% * Prediction

\section{Induction of decision trees}
\label{sec:3:induction}

% * Growing decision trees
%  Description de l'algo (high-level)
%       * Recursive partition
%           Depth/Breadth/Best first
%       * Splitting rules (overview)
%       * Criterion (overview)
%       * Critères d'arrêts [ différents paramètres, chi² ]
%       * Assignment in terminal nodes

\section{Splitting rules}
\label{sec:3:splitting-rules}

%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - set of questions / answers
%               + categorical vs continuous)
%               + bi-partitions vs n-partitions
%               + weak learners
%         - Best splits
%             + Greedy approximation of optimal trees
%             + optimizing locally the impurity <=> optimizing globally? see page 33+
%         - Approximately best splits
%             + Binning
%             + Subsampling

\section{Goodness of split}
\label{sec:3:criteria}

%     > Impurity criteria
%         - Gini, Entropy, Variance, etc (see cart + graphes)
%         - Efficient implementation for iterative evaluation
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)

% \section{Interpreting decision trees}
% % From trees to rules
