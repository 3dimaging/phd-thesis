\chapter{Classification and regression trees}\label{ch:cart}

\section{Introduction}

% * Citer les premiers travaux
%       Historique
%       Présentation unifiée de ID3, C4.5 et CART
% * Review Zighed

\section{Tree structured models}

% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4, Zighed
%     > définitions formelles
%     > Data structures (arrays vs. object-oriented structures)
% * Prediction

\section{Induction of decision trees}

% * Growing decision trees
%     > Recursive partition
%         [ max_depth, max_leaf_nodes, min_samples_split, min_samples_leaf ]
%         - Critères d'arrêt
%         - Depth/Breadth/Best first

\section{Splitting rules}

%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - Best splits
%             + Greedy approximation of optimal trees
%         - Approximately best splits
%             + Binning
%             + Subsampling
%         - Weak learners (different from axis-aligned splits)
%     > Impurity criteria
%         - Gini, Entropy, Variance, etc (see cart + graphes)
%         - Efficient implementation for iterative evaluation
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)

% \section{Interpreting decision trees}
% % From trees to rules
