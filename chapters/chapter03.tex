\chapter{Classification and regression trees}\label{ch:cart}

\section{Introduction}

% * Citer les premiers travaux
%       Historique
%       Présentation unifiée de ID3, C4.5 et CART
% * Review Zighed

\section{Tree structured models}

% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4, Zighed
%     > définitions formelles
%     > Data structures (arrays vs. object-oriented structures)
% * Prediction

\section{Induction of decision trees}

% * Growing decision trees
%  Description de l'algo (high-level)
%       * Recursive partition
%           Depth/Breadth/Best first
%       * Splitting rules (overview)
%       * Criterion (overview)
%       * Critères d'arrêts [ différents paramètres, chi² ]
%       * Assignment in terminal nodes

\section{Splitting rules}

%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - set of questions (categorical vs continuous) + answers (bi-partitions vs n-partitions)
%         - Best splits
%             + Greedy approximation of optimal trees
%         - Approximately best splits
%             + Binning
%             + Subsampling
%         - Weak learners (different from axis-aligned splits)

\section{Goodness of split}

%     > Impurity criteria
%         - Gini, Entropy, Variance, etc (see cart + graphes)
%         - Efficient implementation for iterative evaluation
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)

% \section{Interpreting decision trees}
% % From trees to rules
