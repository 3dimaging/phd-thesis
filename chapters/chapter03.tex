\chapter{Classification and regression trees}\label{ch:cart}

\begin{remark}{Outline}
In this chapter, we present a unified framework in which we detail (single)
decision trees methods. In Section~\ref{sec:3:introduction}, we first give an
overview of the context in which these algorithms have been developed. In
Section~\ref{sec:3:tree-structured-models}, we proceed with a mathematical
presentation, introducing all necessary concepts and notations. The general
learning algorithm is then presented in Section~\ref{sec:3:induction} while
more advanced concepts are discussed in finer details in Sections~\ref{sec:3:splitting-rules}
and \ref{sec:3:criteria}. As such, specific algorithms (e.g.,
CART, ID3 or C4.5) are described as specializations of the general framework
presented here.
\end{remark}

\section{Introduction}
\label{sec:3:introduction}

Since always, artificial intelligence has been driven by the ambition to
understand and uncover complex relations in data. That is, to find models that
can not only produce accurate predictions, but also be used to extract
knowledge in an intelligible way. Guided with this twofold objective, research
in machine learning has given rise to extensive bodies works in a myriad of
directions. Among all of them however, tree-based methods stand as one of
the most effective and useful method, capable to produce both reliable and
understandable results, on mostly any kind of data.

Historically, the appearance of \textit{decision trees} seems to be due to
\citet{morgan:1963}, who first proposed a tree-based method for handling multi-variate
non-additive effects in the context of survey data. Without contest
however, the principal investigators that have driven research on the
methodological principles  are \citet{breiman:1978a,breiman:1978b},
\citet{friedman:1977,friedman:1979} and \citet{quinlan:1979,quinlan:1986} who
simultaneously and independently proposed very close algorithms for the
induction of tree-based models. In particular, the summarizing work of
\citet{breiman:1984}, later complemented with the work of \citet{quinlan:1993},
have set decision trees into a simple and consistent methodological framework,
that have made them easy to understand and easy to use by a large audience. Most
importantly, decision trees are at the foundation of many modern and
state-of-the-art algorithms, including forests of decision trees -- on which this work is
about (see Chapter~\ref{ch:forest}) -- or
boosting methods~\citep{freund:1995,friedman:2001}.

As we will explore in further details all throughout this work, the success
of decision trees (and by extension, of all tree-based methods) may partly
be explained by several factors that make them attractive in practice:
\begin{itemize}
\item[-] Decision trees are non-parametric, i.e., they can model arbitrarily complex relations between inputs and outputs, without any restrictive assumption on their inner structure;
\item[-] Decision trees handle heterogeneous data, including both ordered and categorical variables;
\item[-] Decision trees intrinsically implement feature selection, making them robust to irrelevant or noisy variables;
\item[-] Decision trees are robust to outliers;
\item[-] Decision trees are easily interpretable, even for non-statistically oriented users.
\end{itemize}


\section{Tree structured models}
\label{sec:3:tree-structured-models}

% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4, Zighed
%     > définitions formelles
%     > Data structures (arrays vs. object-oriented structures)
%     > Intuitively, when all variables are ordered, it is like cutting X into rectangles (see CART 31+)
% * Prediction

\section{Induction of decision trees}
\label{sec:3:induction}

% * Growing decision trees
%  Description de l'algo (high-level)
%       * Recursive partition
%           Depth/Breadth/Best first
%       * Splitting rules (overview)
%       * Criterion (overview)
%       * Critères d'arrêts [ différents paramètres, chi² ]
%       * Assignment in terminal nodes

\section{Splitting rules}
\label{sec:3:splitting-rules}

%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - set of questions / answers
%               + categorical vs continuous)
%               + bi-partitions vs n-partitions
%               + weak learners
%         - Best splits
%             + Greedy approximation of optimal trees
%             + optimizing locally the impurity <=> optimizing globally? see page 33+
%         - Approximately best splits
%             + Binning
%             + Subsampling

\section{Goodness of split}
\label{sec:3:criteria}

%     > Impurity criteria
%         - Gini, Entropy, Variance, etc (see cart + graphes)
%         - Efficient implementation for iterative evaluation
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)

% \section{Interpreting decision trees}
% % From trees to rules
