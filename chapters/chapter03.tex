\chapter{Classification and regression trees}\label{ch:cart}

% * Citer les premiers travaux
% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4
%     > Data structures (arrays vs. object-oriented structures)
%     > (Mention multi-output)
% * Growing decision trees
%     > Recursive partition
%         [ max_depth, max_leaf_nodes, min_samples_split, min_samples_leaf ]
%         - Critères d'arrêt
%         - Depth/Breadth/Best first
%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - Best splits
%             + Greedy approximation of optimal trees
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Approximately best splits
%             + Binning
%             + Subsampling
%         - Weak learners (different from axis-aligned splits)
%     > Impurity criteria
%         - Efficient implementation for iterative evaluation
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)
% * Predicting
%     > Procedure

