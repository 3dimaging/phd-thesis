\chapter{Classification and regression trees}\label{ch:cart}

\section{Introduction}

% * Citer les premiers travaux
%       Historique
%       Présentation unifiée de ID3, C4.5 et CART
% * Review Zighed
% * Avantages (cart 56+)
%       - can handle any type of data (both ordered and categorical)
%       - handle conditional information in non-homogeneous relations => consistence
%       - intepretability (cart 58, non-statistical interpretation)
%       - robust to noisy features <=> intrinstic feature selection
%       - invariant with monotone transforme of variables
%       - robust to outliers and misclassification

\section{Tree structured models}

% * Tree structured estimators
%     > (Titanic classification problem)
%     > Estimators as partitions
%         CART page 4, Zighed
%     > définitions formelles
%     > Data structures (arrays vs. object-oriented structures)
%     > Intuitively, when all variables are ordered, it is like cutting X into rectangles (see CART 31+)
% * Prediction

\section{Induction of decision trees}

% * Growing decision trees
%  Description de l'algo (high-level)
%       * Recursive partition
%           Depth/Breadth/Best first
%       * Splitting rules (overview)
%       * Criterion (overview)
%       * Critères d'arrêts [ différents paramètres, chi² ]
%       * Assignment in terminal nodes

\section{Splitting rules}

%     > Splitting nodes
%         [ max_features, min_samples_leaf ]
%         - set of questions / answers
%               + categorical vs continuous)
%               + bi-partitions vs n-partitions
%               + weak learners
%         - Best splits
%             + Greedy approximation of optimal trees
%             + optimizing locally the impurity <=> optimizing globally? see page 33+
%         - Approximately best splits
%             + Binning
%             + Subsampling

\section{Goodness of split}

%     > Impurity criteria
%         - Gini, Entropy, Variance, etc (see cart + graphes)
%         - Efficient implementation for iterative evaluation
%             + How to evaluate multiple candidate thresholds cheaply for
%               a given feature (algorithm, f-ordered)
%             + Sorting algorithms
%         - Effet du critère sur les cuts
%             End-cut preference (CART 11.8)
%             Normalisation par l'entropy (cf. Vincent, Louis)
%             Effet sur la structure des arbres générés
%         - Sample weighting
%             Negative weights? (@ndawe)

% \section{Interpreting decision trees}
% % From trees to rules
