\chapter{Random Forests}\label{ch:forest}

\begin{remark}{Outline}
In this chapter, ...
\end{remark}

\section{Bias-variance tradeoff}

% décomposition de l'erreur en bias+variance/noise
% geurts, 2006, appendix E => est-ce que ça tient la route en termes de correlation effect???

\section{Bagging}

% Décomposition biais-variance
% Geman, Stuart, Elie Bienenstock, and René Doursat. "Neural networks and the bias/variance dilemma." Neural computation 4.1 (1992): 1-58.

    % > Subsampling with replacement => bagging
    % > Deriver l'erreur et montrer qu'on réduit agit sur la variance, biais reste le même
    %   > effet sur la variance (why bagging works, breiman 1996)
    %   > mais correlation effect (hastie)

    % Aggregation
    % > consensus vote
    % > average probability (Hastie, 283+)
    % > average output values

\section{Ensembles of randomized trees}

    % Use the term "random forest" as a global algorithm, as breiman does in RF paper
    % incluant:

    % - Bagging
    % - Random features (Kwok & carter (1990), dietterich 1998 (see rf), breiman)
    % - Random thresholds (geurts)
    % - +other kinds of forests (see state-of-the-art in geurts)
    % => discuter sur un exemple les aspects stat/comput/repres

    % Pq la randomization marche mieux que le bagging seul? (decorrelation)
    % Variance reduction and de-correlation effect due to max_features <= p
    %   aim at low correlation between residuals and low error trees

    % Issues tackled by ensemble
    %   Ensemble methods in ML, Dietterich
    %   - stat
    %   - computational
    %   - representational

    % RF do not overfit (theorem 2.3)

    % OOB estimates

\section{Consistency}

    % > Mise au point
    % > Preuve de consistence des extra-trees?
