\chapter{Random Forests}\label{ch:forest}

\begin{remark}{Outline}
In this chapter, we present a family of ensemble methods known as
\textit{Random Forests}. In Section~\ref{sec:4:bias-variance}, we first
describe the bias-variance decomposition of the prediction error and then
present, in Section~\ref{sec:4:bagging}, how the Bagging ensemble method
reduces the variance term in this decomposition. In
Section~\ref{sec:4:ensemble}, we study Random Forests ant its variants, which
all introduce randomness into the decision tree induction algorithm
to further reduce the prediction error. Finally, the consistency of forests of
randomized trees is explored in Section~\ref{sec:4:consistency}.
\end{remark}

\section{Bias-variance tradeoff}
\label{sec:4:bias-variance}

% décomposition de l'erreur en bias+variance/noise
% geurts, 2006, appendix E => est-ce que ça tient la route en termes de correlation effect???

\section{Bagging}
\label{sec:4:bagging}

% Décomposition biais-variance
% Geman, Stuart, Elie Bienenstock, and René Doursat. "Neural networks and the bias/variance dilemma." Neural computation 4.1 (1992): 1-58.

    % > Subsampling with replacement => bagging
    % > Deriver l'erreur et montrer qu'on réduit agit sur la variance, biais reste le même
    %   > effet sur la variance (why bagging works, breiman 1996)
    %   > mais correlation effect (hastie)

    % Aggregation
    % > consensus vote
    % > average probability (Hastie, 283+)
    % > average output values

\section{Ensembles of randomized trees}
\label{sec:4:ensemble}

    % Use the term "random forest" as a global algorithm, as breiman does in RF paper
    % incluant:

    % - Bagging
    % - Random features (Kwok & carter (1990), dietterich 1998 (see rf), breiman)
    % - Random thresholds (geurts)
    % - +other kinds of forests (see state-of-the-art in geurts)
    % => discuter sur un exemple les aspects stat/comput/repres

    % Pq la randomization marche mieux que le bagging seul? (decorrelation)
    % Variance reduction and de-correlation effect due to max_features <= p
    %   aim at low correlation between residuals and low error trees

    % Issues tackled by ensemble
    %   Ensemble methods in ML, Dietterich
    %   - stat
    %   - computational
    %   - representational

    % RF do not overfit (theorem 2.3)

    % OOB estimates

\section{Consistency}
\label{sec:4:consistency}

    % > Mise au point
    % > Preuve de consistence des extra-trees?
