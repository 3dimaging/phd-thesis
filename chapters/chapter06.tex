\chapter{Variable importances}\label{ch:importances}

\begin{remark}{Outline}
This chapter studies variable importance measures as computed from forests of
randomized trees. In Section~\ref{sec:6:importances}, we first present how
random forests can be used to assess the importance of input variables.  We
then derive in Section~\ref{sec:6:theory} a characterization in asymptotic
conditions and show that variable importances derived from totally randomized trees
offer a three-level decomposition of the information jointly  contained in the
input variables about the output. In Section~\ref{sec:6:variable-relevance}, we
show that this  characterization only depends on the relevant variables and
then discuss these ideas in Section~\ref{sec:6:variants} in the context of
variants closer to the Random Forest algorithm. Finally, we illustrate these
results on an artifical problem in Section~\ref{sec:6:illustration}.
\end{remark}

An important task in many scientific fields is the prediction of  a response
variable based on a set of predictor variables. In many situations though, the
aim is not only to make the most accurate predictions of the response but also
to identify which predictor variables are the most important to make these
predictions, e.g. in order to understand the underlying process. Because of
their applicability to a wide range of problems and their capability to both
build accurate models and, at the same time, to provide variable importance
measures, random forests have become a major data analysis tool used with
success in various scientific areas.

Despite their extensive use in applied research, only a couple of works have
studied the theoretical properties and statistical mechanisms of these
algorithms. \citet{zhao:2000}, \citet{breiman:2004},
\citet{biau:2008,biau:2012}, \citet{meinshausen:2006} and \citet{lin:2006}
investigated simplified to very realistic variants of these algorithms and
proved  the consistency of those variants. Little is known however regarding
the variable importances computed by random forests, and -- as far as we know
-- the work of~\citet{ishwaran:2007} is indeed the only theoretical study of
tree-based variable importance measures. In this chapter, we aim at filling
this gap and present a theoretical  analysis of the Mean Decrease Impurity
importance derived from ensembles of randomized trees.

\textit{The content of this chapter is based on previous work published in \citep{louppe:2013}.}

% A forest of trees is impenetrable
% as far as simple interpretations of its mechanims go~\citep{breiman:2001}.


\section{Variable importances}
\label{sec:6:importances}

\subsection{Importances in single decision trees}

In the context of single decision trees, \cite{breiman:1984} first defined
the measure of importance of a variable $X_j$ as
\begin{equation}
\text{Imp}(X_j) = \sum_{t\in \varphi} \Delta I(\tilde{s}^j_t, t),
\end{equation}
where $\tilde{s}^j_t$ is the best surrogate split
for $s_t$ -- that is the best split using variable $X_j$ to predict the actual
split $s_t$ defined at node $t$. The use of surrogate splits was proposed to
account for masking effects: it may indeed happen that some variable $X_{j_2}$
never occurs in any split because it leads to splits that are slightly worse,
and therefore not selected, than those of some other variable $X_{j_1}$.
However, if $X_{j_1}$ is removed and another tree is grown, $X_{j_2}$ may now
occur prominently within the splits and the resulting tree may be nearly as good
as the original tree. In such a case, a relevant measure should detect the
importance of $X_{j_2}$. Accordingly, if $X_{j_2}$ is being masked at $t$ by
$X_{j_1}$ (i.e., if $X_{j_1}$ is used to split $t$), but if $\tilde{s}^{j_2}_t$ is similar to
$s_t$, but not quite as good, then $\Delta I(\tilde{s}^{j_2}_t, t)$ will be
nearly as large as $\Delta I(s_t, t)$ and therefore the proposed measure will
indeed account for the importance of $X_{j_2}$.

Thanks to randomization, masking effects are dampened within forests of
randomized trees. Even if $X_{j_2}$ is being masked by $X_{j_1}$ there is indeed
still a chance for $X_{j_2}$ to be chosen as a split if $X_{j_1}$ is not
selected among the $K$ variables chosen at random. Depending on the value $K$,
masking effects do not disappear entirely though. The use of bootstrap samples
also helps reduce masking effects, making $X_{j_1}$ or $X_{j_2}$ just slightly
better than the other due to the variations in the bootstrap samples.

\subsection{Importances in forests}

In the context of ensembles of randomized trees,
\cite{breiman:2001,breiman:2002} proposed to evaluate the importance of
a variable $X_j$  for predicting  $Y$ by adding up the weighted impurity decreases $p(t) \Delta
i(s_t, t)$ for all nodes $t$ where $X_j$ is used, averaged over all trees $\varphi_m$ (for $m=1,\dots,M$)
in the forest:
\begin{equation}\label{eq:mdi}
\text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_{m}} 1(X_{s_t} = X_j) \Big[ p(t) \Delta i(s_t, t) \Big],
\end{equation}
where $p(t)$ is the proportion $\tfrac{N_t}{N}$ of samples
reaching $t$ and where $X_{s_t}$ denotes the variable used in split $s_t$. When
using the Gini index as impurity function, this measure is known as the
\textit{Gini importance} or \textit{Mean Decrease Gini}. However, since it can
be defined for any impurity measure $i(t)$, we will refer to Equation~\ref{eq:mdi}
as the \textit{Mean Decrease Impurity} importance (MDI), no matter the impurity
measure $i(t)$. We will characterize and derive results for this measure in the
rest of this text.

In addition to MDI, \cite{breiman:2001,breiman:2002} also proposed to evaluate
the importance of a variable $X_j$ by measuring the \textit{Mean Decrease
Accuracy} (MDA) of the forest when the values of $X_j$ are randomly permuted in
the out-of-bag samples. For that reason, this
latter measure is also known as the \textit{permutation importance}. Formally,
in regression, the permutation importance of $X_j$ is given as:
\begin{align}\label{eq:mda}
\text{Imp}(X_j) &= \frac{1}{N} \sum_{(\mathbf{x}_i,y_i) \in {\cal L}} L(\frac{1}{M^{-i}} \sum_{l=1}^{M^{-i}} \varphi_{{\cal L}^{m_{k_l}}}(\mathbf{x}_i), y_i) \\ \nonumber
                &\quad - \frac{1}{N} \sum_{(\mathbf{x}_i^\prime,y_i) \in \pi_j({\cal L})} L(\frac{1}{M^{-i}} \sum_{l=1}^{M^{-i}} \varphi_{{\cal L}^{m_{k_l}}}(\mathbf{x}_i^\prime), y_i)
\end{align}
where $\pi_j({\cal L})$ denotes a replicate of ${\cal L}$ in which in the
values of $X_j$ have been randomly permuted. For classification, the
permutation importance is derived similarly as in Equation~\ref{eq:mda},
except  that the out-of-bag average predictions are
replaced with the class which is the most likely, as computed from the out-of-bag
class probability estimates. Its rationale is that randomly permuting the
input variable $X_j$ should break its association with the response $Y$. Therefore,
if $X_j$ is in fact associated to $Y$, permuting its values should also result
in a substantial decrease of accuracy, as here measured by the difference between
the out-of-bag estimates of the generalization error.

Thanks to popular machine learning
softwares~\citep{breiman:2002,liaw:2002,pedregosa:2011},
both of these variable importance measures have shown their practical utility in
an increasing number of experimental studies. Little is known however regarding
their inner workings. \cite{strobl:2007b} compare both MDI and MDA and show
experimentally that the former is biased towards some predictor variables. As
explained by~\cite{white:1994} in case of single decision trees, this
bias stems from an unfair advantage given by the usual impurity functions $i(t)$
towards predictors with a large number of values. \cite{strobl:2008}
later showed  that MDA is biased as well, and that it overestimates the
importance of correlated variables -- although this effect was not confirmed in
a later experimental study by~\cite{genuer:2010}.  From a theoretical
point of view, \cite{ishwaran:2007} provides a detailed theoretical
development of a simplified version of MDA, giving key insights for the
understanding of the actual MDA.

\section{Theoretical study}
\label{sec:6:theory}

\subsection{Background}

To be self-contained, we first recall several definitions from information
theory (see \citep{cover:2012}, for further properties).

We suppose that we are given a probability space $(\Omega, {\cal E},
\mathbb{P})$ and  consider random variables defined on it taking a finite
number of possible values. We use upper case letters to denote such random
variables (e.g. $X, Y, Z, W \ldots$)  and calligraphic letters (e.g. $\cal X,
Y, Z, W \ldots$) to denote their image sets (of finite cardinality), and lower
case letters (e.g. $x, y, z, w \ldots$) to denote one of their possible values.
For a (finite) set of (finite) random variables $ X = \{X_{1}, \ldots ,
X_{p}\}$, we denote by $P_{X}(x) = P_{X}(x_{1}, \ldots , x_{p})$ the
probability $\mathbb{P}(\{ \omega \in \Omega \mid  \forall \ell : 1, \ldots, p:
X_{\ell}(\omega) =x_{\ell}\})$, and by ${\cal X} = {\cal X}_{1} \times \cdots
\times {\cal X}_{p}$ the set of joint configurations of these random variables.
Given two sets of random variables, $X = \{X_{1}, \ldots , X_{p}\}$ and
$Y=\{Y_{1}, \ldots , Y_{q}\}$, we denote by $P_{X \mid Y}(x \mid y) = {P_{X, Y}
(x,  y)}/ {P_{Y}(y)}$ the conditional density of $X$ with respect to
$Y$.\footnote{To avoid problems, we suppose that all probabilities are strictly
positive, without fundamental limitation.}

With these notations, the joint (Shannon) entropy of a set of random variables
$X =\{X_{1}, \ldots , X_{p}\}$ is thus defined by
\begin{equation}
H(X)  = - \sum_{x \in {\cal X}}P_{X} (x)\log_{2}P_{X }(x),
\end{equation}
while the mean conditional entropy of a set of random variables $X = \{X_{1},
\ldots , X_{p}\}$, given the values of another set of random variables
$Y=\{Y_{1}, \ldots , Y_{q}\}$ is defined by
\begin{equation}
H(X\mid Y) = - \sum_{x \in {\cal X}} \sum_{y \in {\cal Y}} P_{X, Y} (x, y) \log_{2} P_{X \mid Y} (x  \mid y).
 \end{equation}
The mutual information among the set of random variables $X =\{X_{1}, \ldots ,
X_{p}\}$ and the set of random variables $Y=\{Y_{1}, \ldots , Y_{q}\}$ is
defined by
 \begin{align}
 I(X; Y) &= - \sum_{x \in {\cal X}} \sum_{y \in {\cal Y}} P_{X, Y} (x, y) \log_{2} \frac{P_{X}(x) P_{Y}(y)}{P_{X,Y}(x,y)} \\
 &= H(X) - H(X \mid Y) \nonumber \\
 &=  H(Y) - H(Y \mid X) \nonumber
 \end{align}

The mean conditional mutual information among the set of random variables $X
=\{X_{1}, \ldots , X_{p}\}$ and the set of random variables $Y=\{Y_{1}, \ldots
, Y_{q}\}$, given the values of a third set of random variables $Z=\{Z_{1},
\ldots , Z_{r}\}$, is defined by
 \begin{align}
 I(X; Y \mid Z) &= H(X \mid Z) - H(X \mid Y, Z)\\
 & = H(Y \mid Z) - H(Y \mid X, Z) \nonumber\\
& = - \sum_{x \in {\cal X}} \sum_{y \in {\cal Y}} \sum_{z \in {\cal Z}} P_{X, Y, Z} (x, y, z) \log_{2} \frac{P_{X \mid Z}(x \mid z) P_{Y\mid Z}(y \mid z)}{P_{X,Y \mid Z}(x,y \mid z)}\nonumber
\end{align}

We also recall the chaining rule
\begin{equation}
I(X, Z ; Y \mid W ) = I(X; Y \mid W  ) + I( Z ; Y \mid W, X),
\end{equation}
and the symmetry of the (conditional) mutual information among sets of random variables
\begin{equation}
I(X ; Y \mid Z) = I(Y ;  X  \mid Z).
\end{equation}


\subsection{Asymptotic analysis}

Let us now consider the MDI  importance as defined by Equation~\ref{eq:mdi},
and let us assume a set $V= \{X_1, ..., X_p\}$  of {\em categorical} input
variables and a {\em categorical} output $Y$. For the sake of simplicity we
will  use the Shannon entropy as impurity measure, and focus on totally
randomized trees; later on we will discuss other impurity measures and tree
construction algorithms.

Given a training sample ${\cal L}$ of $N$ joint observations of $X_1, ..., X_p,
Y$ independently drawn from the joint distribution $P(X_1, ..., X_p, Y)$, let us
assume that we infer from it an infinitely large ensemble of \textit{totally
randomized and fully developed trees}. In this setting, a totally randomized and
fully developed tree is defined as a decision tree in which each node $t$ is
partitioned using a variable $X_j$ picked uniformly at random among those not
yet used at the parent nodes of $t$, and where each $t$ is split into $|{\cal
X}_j|$ sub-trees, i.e., one for each possible value of ${\cal X}_j$, and where
the recursive construction process halts only when all $p$ variables have been
used along the current branch.  Hence, in such a tree, leaves are all at the
same depth $p$, and the set of leaves of a fully developed tree is in bijection
with the set $\cal X$ of all possible joint configurations of the $p$ input
variables. For example, if the input variables are all binary, the resulting
tree will have exactly $2^{p}$ leaves.

\begin{theorem}\label{thm:imp}
The MDI importance of $X_j \in V$ for $Y$ as computed
with an   infinite ensemble of fully developed totally randomized trees and an
infinitely large training sample is:
  \begin{equation}\label{eqn:imp-full}
  \text{Imp}(X_j)=\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B),
  \end{equation}
\end{theorem}
\noindent where $V^{-j}$ denotes the subset $V \setminus \{X_j\}$, ${\cal
P}_k(V^{-j})$ is the set of subsets of  $V^{-j}$ of cardinality $k$, and
$I(X_j;Y|B)$ is the conditional mutual information of $X_{j}$ and $Y$ given the
variables in $B$.

\begin{proof}
By expanding $\Delta i(s, t) = i(t) - p_L i(t_L) - p_R i(t_R)$ into
Equation~\ref{eq:mdi} and using the entropy $H(Y|t) = -\sum_j p(j|t) \log_2(p(j|t))$
as impurity measure $i(t)$, Equation~\ref{eq:mdi} can be rewritten in terms of
mutual information:
\begin{equation}
\text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_m} 1(X_{s_t} = X_j) p(t) I(Y;X_j|t)
\end{equation}

As the size $N$ of the training sample grows to infinity, $p(t)$ becomes the
(exact) probability (according to $P(X_1,\ldots,X_p,Y)$) that an object reaches
node $t$, i.e., a probability $P(B(t)=b(t))$ where $B(t)=(X_{i_1}, ...,
X_{i_{k}})$ is the subset of $k$ variables tested in the branch from the root
node to the parent of $t$ and $b(t)$ is the vector of values of these
variables. As the the number $M$ of totally randomized trees also grows to
infinity, the importance of a variable $X_j$ can then be written:
\begin{equation}
\text{Imp}(X_j)=\sum_{B\subseteq V^{-j}} \sum_{b\in {\cal X}_{i_1}\times ... \times {\cal X}_{i_{k}} } \alpha(B,b,X_j,p) P(B=b) I(Y;X_j|B=b),
\end{equation}
where $b$ is a set of values for the variables in $B$ and $\alpha(B,b,X_j,p)$ is
the probability that a node $t$ (at depth $k$) in a totally randomized tree
tests the variable $X_j$ and is such that $B(t)=B$ and $b(t)=b$.

Let us compute $\alpha(B,b,X_j,p)$. First, let us consider the probability that
a node $t$ tests the variable $X_j$ and is such that the branch leading to $t$
follows a path defined, in that particular order, by all $k$ variables $X_{i_1},
..., X_{i_{k}} \in B$ and their corresponding values in $b$. The probability of
that branch is the probability of picking (uniformly at random)  $X_{i_1}$  at
the root node times the probability of testing, in that order, the remaining
$X_{i_2}, ..., X_{i_{k}}$ variables in the sub-tree corresponding to the value
$x_{i_1}$ of $X_{i_1}$ defined in $b$. Note that, by construction, it is certain
that this particular sub-tree exists since the root node is split into $|{\cal
X}_{i_1}|$ sub-trees.  Then, the
probability of testing $X_j$ at the end of this branch is the probability of
picking $X_j$ among the remaining $p-k$ variables. By recursion, we thus have:
\begin{equation}
\frac{1}{p} \frac{1}{p-1} ... \frac{1}{p-k+1} \frac{1}{p-k} = \frac{(p-k)!}{p!} \frac{1}{p-k}
\end{equation}

Since the order along which the variables appear in the branch is of no
importance, $\alpha(B,b,X_j,p)$ actually includes all $k!$ equiprobable ways of
building a branch composed of the variables and values in $B$ and $b$. Then,
since a tree may at most contain a single such branch, whatever the order of the
tests, the probabilities may be added up and it comes:
\begin{equation}
\alpha(B,b,X_j,p)=k! \frac{(p-k)!}{p!} \frac{1}{p-k} = \frac{1}{C_p^k} \frac{1}{p-k}
\end{equation}

From the above expression, it appears that $\alpha(B,b,X_j,p)$ depends only on
the size $k$ of $B$ and on the number $p$ of variables. As such, by grouping in
the previous equation of $\text{Imp}(X_j)$ conditioning variable subsets $B$ according
to their sizes and using the definition of conditional mutual information,
$\alpha$ can be factored out, hence leading to the form foretold by Theorem~\ref{thm:imp}:
\begin{equation}
\text{Imp}(X_j)=\sum_{k=0}^{p-1} \frac{1}{C^k_p} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B).
\end{equation}
\end{proof}

\begin{theorem}\label{thm:sum-of-imp}
For any ensemble of fully developed trees in asymptotic learning sample size
conditions (e.g., in the same conditions as those of Theorem~\ref{thm:imp}), we
have that
\begin{equation}\label{eqn:sum-of-imp}
\sum_{j=1}^{p} \text{Imp}(X_j) = I(X_{1}, \ldots, X_{p} ; Y).
\end{equation}
\end{theorem}

\begin{proof}
For any tree $\varphi$, we have that the sum of all importances estimated by
using an infinitely large sample $\cal L$ (or equivalently, by assuming perfect
knowledge of the joint distribution $P(X_1, ..., X_p, Y)$) is equal to $H(Y) -
\sum_{t \in \widetilde{\varphi})} p(t) H(Y|b(t))$, where $\widetilde{\varphi}$
denotes the set of all leaves of $\varphi$, and where $b(t)$  denotes the joint
configuration of all input variables leading to leaf $t$. This is true because
the impurities of all test nodes intervening in the computation of the variable
importances, except the impurity $H(Y)$ at the root node of the tree, cancel
each other when summing up the importances.

Since, when the tree is fully developed, $\sum_{t \in \widetilde{\varphi}} p(t)
H(Y|b(t))$ is obviously equal to the mean conditional entropy $H(Y | X_{1},
\ldots, X_{p})$ of $Y$ given all input variables, this implies that for any
fully developed tree we have that the sum of variable importances is equal to
$I(X_{1}, \ldots, X_{p} ; Y)$, and so this relation also holds when averaging
over an infinite ensemble of totally randomized trees.
\end{proof}

Together, theorems \ref{thm:imp} and \ref{thm:sum-of-imp} show that  variable
importances derived from totally randomized trees in asymptotic conditions
provide a three-level decomposition of the information $I(X_{1}, \ldots, X_{p}
; Y)$ contained in the set of input variables about the output variable. The
first level is a decomposition among input variables (see Equation~\ref{eqn:sum-of-imp}
of Theorem~\ref{thm:sum-of-imp}),  the second level is a
decomposition along the degrees $k$ of interaction terms of a variable with the
other ones (see the outer sum in Equation~\ref{eqn:imp-full} of
Theorem~\ref{thm:imp}), and the third level is a decomposition along the
combinations $B$ of interaction terms of fixed size $k$ of possible interacting
variables (see the inner sum in Equation~\ref{eqn:imp-full}).

We observe that the decomposition includes, for each variable, each and every
interaction term of each and every degree weighted in a fashion resulting only
from the combinatorics of possible interaction terms. In particular, since all
$I(X_j;Y|B)$ terms are at most equal to $H(Y)$, the prior entropy of $Y$,  the
$p$ terms of the outer sum of Equation~\ref{eqn:imp-full} are each upper
bounded by
\begin{equation}
\frac{1}{C^k_p}\frac{1}{p-k}\sum_{B \in {\cal P}_k(V^{-j})}
H(Y)=\frac{1}{C^k_p} \frac{1}{p-k} {C^k_{p-1}} H(Y) = \frac{1}{p}H(Y).
\end{equation}
As such,
the second level decomposition resulting from totally randomized trees makes the
$p$ sub-importance terms
\begin{equation}
\frac{1}{C^k_p} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{equation}
to equally contribute (at most) to the total
importance, even though they each include a combinatorially different number of
terms.


\section{Relevance of variables}
\label{sec:6:variable-relevance}

Following~\citet{kohavi:1997}, let us define as {\em relevant to $Y$ with
respect to $V$} a variable $X_jm$ for which there exists at least one subset $B
\subseteq V$ (possibly empty) such that $I(X_j;Y|B)>0$. Thus we define as {\em
irrelevant to $Y$ with respect to $V$} a variable $X_i$ for which, for all $B
\subseteq V$, $I(X_i; Y|B)=0$. Remark that if $X_i$ is irrelevant to $Y$ with
respect to $V$, then by definition it is also irrelevant to $Y$ with respect to
any subset of $V$. However, if $X_j$ is relevant to $Y$ with respect to $V$,
then it is not necessarily relevant to $Y$ with respect to all  subsets of $V$.
Among the relevant variables, we also distinguish the {\em marginally} relevant
ones, for which $I(X_{j}; Y) > 0$, the  {\em strongly} relevant ones, for which
$I(X_{j}; Y | V^{-j}) > 0$,  and the {\em weakly} relevant variables, which are
relevant but not strongly relevant.

\begin{theorem}\label{thm:irrelevant}
  $X_i \in V$ is irrelevant to $Y$ with respect to $V$ if and only if  its
  infinite sample size importance as computed with an infinite ensemble of fully
  developed totally randomized trees built on $V$ for $Y$ is 0.
\end{theorem}

\begin{proof}
The proof directly results from the definition of irrelevance. If $X_i$ is
irrelevant with respect to $V$, then $I(X_i;Y|B)$ is zero for all $B \subseteq
V^{-i} \subset V$ and Equation~\ref{eqn:imp-full} reduces to $0$. Also, since
$I(X_i;Y|B)$ is non-negative for any $B$, $\text{Imp}(X_i)$ is zero if and only if all
its $I(X_i;Y|B)$ terms are zero. Since $\text{Imp}(X_i)$ includes all $I(X_i;Y|B)$
terms for $B \subseteq V^{-i}$, and since all of them are therefore null if
$\text{Imp}(X_i)=0$,  $X_i$ is thus, by definition, irrelevant with respect to
$V^{-i}$. $X_i$ is then also trivially irrelevant with respect to $V=V^{-i} \cup
\{X_i\}$ since $I(X_i;Y|B\cup\{X_i\})=0$ for any $B$.
\end{proof}

\begin{lemma}\label{lemma:adding-irrelevant}
  Let $X_i \notin V$ be an irrelevant variable for $Y$ with respect to $V$. The infinite
  sample size importance of $X_j \in V$ as computed with an infinite
  ensemble of fully developed totally randomized trees built on $V$ for $Y$ is the
  same as the importance derived when using $V\cup \{X_i\}$ to build the ensemble of trees for $Y$.
\end{lemma}

\begin{proof}
Let $X_i \notin V$ be an irrelevant variable with respect to $V$. For $X_j \in
V$, $B \subseteq V^{-j}$, using the chain rules of mutual information, we have:
\begin{align}
I(X_j, X_i;Y|B) &= I(X_j;Y|B) + I(X_i;Y|B \cup \{X_j\})  \\
                &= I(X_i;Y|B) + I(X_j;Y|B \cup \{X_i\})
\end{align}
If $X_i$ is irrelevant with respect to $V$, i.e., such that $I(X_i;Y|B)=0$ for
all $B\subseteq V$, then $I(X_i;Y|B \cup \{X_j\})$ and $I(X_i;Y|B)$ both equal
$0$, leading to
\begin{equation}
I(X_j;Y|B \cup \{X_i\}) = I(X_j;Y|B)
\end{equation}

Then, from Theorem~\ref{thm:imp}, the importance of $X_j$ as computed with an
infinite ensemble of totally randomized trees built on $V\cup \{X_i\}$ can be
simplified to:
\begin{align}
  Imp(X_j)&=\sum_{k=0}^{p-1+1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j} \cup \{X_i\})} I(X_j;Y|B)\nonumber\\
          &=\sum_{k=0}^{p} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \left[ \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B) + \sum_{B \in {\cal P}_{k-1}(V^{-j})} I(X_j;Y|B \cup \{X_i\})  \right] \nonumber\\
          &=\sum_{k=0}^{p-1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B) + \nonumber\\
          & \hookrightarrow \sum_{k=1}^{p} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_{k-1}(V^{-j})} I(X_j;Y|B)\nonumber\\
          &=\sum_{k=0}^{p-1} \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B) + \nonumber\\
          & \hookrightarrow \sum_{k'=0}^{p-1} \frac{1}{C_{p+1}^{k'+1}} \frac{1}{p+1-k'-1} \sum_{B \in {\cal P}_{k'}(V^{-j})} I(X_j;Y|B)\nonumber\\
          &=\sum_{k=0}^{p-1} \left[ \frac{1}{C_{p+1}^k} \frac{1}{p+1-k} + \frac{1}{C_{p+1}^{k+1}} \frac{1}{p-k} \right] \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)\nonumber\\
          &=\sum_{k=0}^{p-1} \frac{1}{C^k_p} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{align}
The last line above exactly corresponds to the importance of $X_j$ as computed
with an infinite ensemble of totally randomized trees built on $V$, which proves
Lemma~\ref{lemma:adding-irrelevant}.
\end{proof}

\begin{theorem}\label{thm:relevant}
  Let $V_R \subseteq V$ be the subset of all variables in $V$ that are relevant to $Y$ with
  respect to $V$. The infinite sample size importance of any variable $X_j \in
  V_R$ as computed with an infinite ensemble of fully developed totally randomized
  trees built on $V_R$ for $Y$ is the same as its importance computed in the same conditions by using all variables in $V$. That is:
    \begin{align}
      \text{Imp}(X_j)&=\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)\nonumber \\
                     &=\sum_{l=0}^{r-1} \frac{1}{C_r^l} \frac{1}{r-l} \sum_{B \in {\cal P}_l(V_R^{-j})} I(X_j;Y|B)
    \end{align}
  where $r$ is the number of relevant variables in $V_R$.
\end{theorem}

\begin{proof}
Let us assume that $V_R$ contains $r \leq p$ relevant variables. If an infinite
ensemble of totally randomized trees were to be built directly on those $r$
variables then, from Theorem~\ref{thm:imp}, the importance of a relevant
variable $X_j$ would be:
\begin{equation}%\label{eqn:imp-rel}
  Imp(X_j)=\sum_{l=0}^{r-1} \frac{1}{C_r^l} \frac{1}{r-l} \sum_{B \in {\cal P}_l(V_R^{-m})} I(X_j;Y|B)
\end{equation}

Let $X_i \in V \setminus V_R$ be one of the $p-r$ irrelevant variables in $V$
with respect to $V$. Since $X_i$ is also irrelevant with respect to $V_R$, using
Lemma~\ref{lemma:adding-irrelevant}, the importance of $X_j$ when the ensemble
is built on $V_R \cup \{X_i\}$ is the same as the one computed on $V_R$ only
(i.e., as computed by the equation above). Using the same argument,
adding a second irrelevant variable $X_{i'}$ with respect to $V$ -- and therefore
also with respect to $V_R \cup \{X_i\}$ -- and building an ensemble of totally
randomized trees on $V_R \cup \{X_i\} \cup \{X_{i'}\}$ will yield importances
that are the same as those computed on $V_R \cup \{X_i\}$, which are themselves
the same as those computed by an ensemble built on $V_R$. By induction, adding
all $p-r$ irrelevant variables has therefore no effect on the importance of
$X_j$, which means that:
\begin{align}%\label{eqn:imp-rel-equiv}
  \text{Imp}(X_j)&=\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)\nonumber \\
          &=\sum_{l=0}^{r-1} \frac{1}{C_r^l} \frac{1}{r-l} \sum_{B \in {\cal P}_l(V_R^{-m})} I(X_j;Y|B)
\end{align}
\end{proof}

Theorems~\ref{thm:irrelevant} and \ref{thm:relevant} show that the
importances computed with an ensemble of totally randomized trees depends only
on the relevant variables. Irrelevant variables have a zero importance and do
not affect the importance of relevant variables. Practically, we believe that
such properties are desirable conditions for a sound criterion assessing the
importance of a variable. Indeed, noise should not be credited of any importance
and should not make any other variable more (or less) important.

Intuitively, the independence with respect to irrelevant variables can be partly
attributed to the fact that splitting at $t$ on some irrelevant variable $X_i$
should only dillute the local importance $p(t) \Delta i(t)$ of a relevant
variable $X_j$ into the children $t_L$ and $t_R$, but not affect the total sum.
For instance, if $X_j$ was to be used at $t$, then the local importance would be
proportional to $p(t)$. By contrast, if $X_i$ was to be used at $t$ and $X_j$ at
$t_L$ and $t_R$, then the sum of the local importances for $X_j$ would be
proportional to $p(t_L) + p(t_R)=p(t)$, which does not change anything.
Similarly, one can recursively invoke the same argument if $X_j$ was to be used
deeper in $t_L$ or $t_R$.

A second reason comes from the
fact that local importances are collected only in nodes $t$ where $X_j$ is used.
By contrast, if local importances were summed over all nodes (e.g., using
surrogate splits), then it would necessarily depend on the total number of nodes
in a tree, which itself directly depends on $p$ -- that is, not on $r$.

Finally, it is also worth noting that this result is consistent with the work
of~\citet{biau:2012}, who proved that rate of convergence of forests
of randomized also only depends on the relevant variables.


\section{Variable importances in random forest variants}
\label{sec:6:variants}


\section{Illustration}
\label{sec:6:illustration}

