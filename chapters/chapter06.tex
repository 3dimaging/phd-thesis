\chapter{Variable importances}\label{ch:importances}

\begin{remark}{Outline}
This chapter studies variable importance measures as computed from forests of
randomized trees. In Section~\ref{sec:6:importances}, we first present how
random forests can be used to assess the importance of input variables.  We
then derive in Section~\ref{sec:6:theory} a characterization in asymptotic
conditions and show that variable importances derived from totally randomized trees
offer a three-level decomposition of the information jointly  contained in the
input variables about the output. In Section~\ref{sec:6:variable-relevance}, we
show that this  characterization only depends on the relevant variables and
then discuss these ideas in Section~\ref{sec:6:variants} in the context of
variants closer to the Random Forest algorithm. Finally, we illustrate these
results on an artifical problem in Section~\ref{sec:6:illustration}.
\end{remark}

An important task in many scientific fields is the prediction of  a response
variable based on a set of predictor variables. In many situations though, the
aim is not only to make the most accurate predictions of the response but also
to identify which predictor variables are the most important to make these
predictions, e.g. in order to understand the underlying process. Because of
their applicability to a wide range of problems and their capability to both
build accurate models and, at the same time, to provide variable importance
measures, random forests have become a major data analysis tool used with
success in various scientific areas.

Despite their extensive use in applied research, only a couple of works have
studied the theoretical properties and statistical mechanisms of these
algorithms. \citet{zhao:2000}, \citet{breiman:2004},
\citet{biau:2008,biau:2012}, \citet{meinshausen:2006} and \citet{lin:2006}
investigated simplified to very realistic variants of these algorithms and
proved  the consistency of those variants. Little is known however regarding
the variable importances computed by random forests, and -- as far as we know
-- the work of~\citet{ishwaran:2007} is indeed the only theoretical study of
tree-based variable importance measures. In this chapter, we aim at filling
this gap and present a theoretical  analysis of the Mean Decrease Impurity
importance derived from ensembles of randomized trees.

\textit{The content of this chapter is based on previous work published in \citep{louppe:2013}.}

% A forest of trees is impenetrable
% as far as simple interpretations of its mechanims go~\citep{breiman:2001}.


\section{Variable importances}
\label{sec:6:importances}

\subsection{Importances in single decision trees}

In the context of single decision trees, \cite{breiman:1984} first defined
the measure of importance of a variable $X_j$ as
\begin{equation}
\text{Imp}(X_j) = \sum_{t\in \varphi} \Delta I(\tilde{s}^j_t, t),
\end{equation}
where $\tilde{s}^j_t$ is the best surrogate split
for $s_t$ -- that is the best split using variable $X_j$ to predict the actual
split $s_t$ defined at node $t$. The use of surrogate splits was proposed to
account for masking effects: it may indeed happen that some variable $X_{j_2}$
never occurs in any split because it leads to splits that are slightly worse,
and therefore not selected, than those of some other variable $X_{j_1}$.
However, if $X_{j_1}$ is removed and another tree is grown, $X_{j_2}$ may now
occur prominently within the splits and the resulting tree may be nearly as good
as the original tree. In such a case, a relevant measure should detect the
importance of $X_{j_2}$. Accordingly, if $X_{j_2}$ is being masked at $t$ by
$X_{j_1}$ (i.e., if $X_{j_1}$ is used to split $t$), but if $\tilde{s}^{j_2}_t$ is similar to
$s_t$, but not quite as good, then $\Delta I(\tilde{s}^{j_2}_t, t)$ will be
nearly as large as $\Delta I(s_t, t)$ and therefore the proposed measure will
indeed account for the importance of $X_{j_2}$.

Thanks to randomization, masking effects are dampened within forests of
randomized trees. Even if $X_{j_2}$ is being masked by $X_{j_1}$ there is indeed
still a chance for $X_{j_2}$ to be chosen as a split if $X_{j_1}$ is not
selected among the $K$ variables chosen at random. Depending on the value $K$,
masking effects do not disappear entirely though. The use of bootstrap samples
also helps reduce masking effects, making $X_{j_1}$ or $X_{j_2}$ just slightly
better than the other due to the variations in the bootstrap samples.

\subsection{Importances in forests}

In the context of ensembles of randomized trees,
\cite{breiman:2001,breiman:2002} proposed to evaluate the importance of
a variable $X_j$  for predicting  $Y$ by adding up the weighted impurity decreases $p(t) \Delta
i(s_t, t)$ for all nodes $t$ where $X_j$ is used, averaged over all trees $\varphi_m$ (for $m=1,\dots,M$)
in the forest:
\begin{equation}\label{eq:mdi}
\text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_{m}} 1(X_{s_t} = X_j) \Big[ p(t) \Delta i(s_t, t) \Big],
\end{equation}
where $p(t)$ is the proportion $\tfrac{N_t}{N}$ of samples
reaching $t$ and where $X_{s_t}$ denotes the variable used in split $s_t$. When
using the Gini index as impurity function, this measure is known as the
\textit{Gini importance} or \textit{Mean Decrease Gini}. However, since it can
be defined for any impurity measure $i(t)$, we will refer to Equation~\ref{eq:mdi}
as the \textit{Mean Decrease Impurity} importance (MDI), no matter the impurity
measure $i(t)$. We will characterize and derive results for this measure in the
rest of this text.

In addition to MDI, \cite{breiman:2001,breiman:2002} also proposed to evaluate
the importance of a variable $X_j$ by measuring the \textit{Mean Decrease
Accuracy} (MDA) of the forest when the values of $X_j$ are randomly permuted in
the out-of-bag samples. For that reason, this
latter measure is also known as the \textit{permutation importance}. Formally,
in regression, the permutation importance of $X_j$ is given as:
\begin{align}\label{eq:mda}
\text{Imp}(X_j) &= \frac{1}{N} \sum_{(\mathbf{x}_i,y_i) \in {\cal L}} L(\frac{1}{M^{-i}} \sum_{l=1}^{M^{-i}} \varphi_{{\cal L}^{m_{k_l}}}(\mathbf{x}_i), y_i) \\ \nonumber
                &- \frac{1}{N} \sum_{(\mathbf{x}_i^\prime,y_i) \in \pi_j({\cal L})} L(\frac{1}{M^{-i}} \sum_{l=1}^{M^{-i}} \varphi_{{\cal L}^{m_{k_l}}}(\mathbf{x}_i^\prime), y_i)
\end{align}
where $\pi_j({\cal L})$ denotes the replicate of ${\cal L}$ in which in the
values of $X_j$ have been randomly permuted. For classification, the
permutation importance is derived similarly as in Equation~\ref{eq:mda},
except  that the out-of-bag average predictions are
replaced with the class which is the most likely, as computed from the out-of-bag
class probability estimates. Its rationale is that randomly permuting the
input variable $X_j$ should break its association with the response $Y$. Therefore,
if $X_j$ is in fact associated to $Y$, permuting its values should also result
in a substantial decrease of accuracy, as here measured by the difference between
out-of-bag estimates.

Thanks to popular machine learning
softwares~\citep{breiman:2002,liaw:2002,pedregosa:2011},
both of these variable importance measures have shown their practical utility in
an increasing number of experimental studies. Little is known however regarding
their inner workings. \cite{strobl:2007b} compare both MDI and MDA and show
experimentally that the former is biased towards some predictor variables. As
explained by~\cite{white:1994} in case of single decision trees, this
bias stems from an unfair advantage given by the usual impurity functions $i(t)$
towards predictors with a large number of values. \cite{strobl:2008}
later showed  that MDA is biased as well, and that it overestimates the
importance of correlated variables -- although this effect was not confirmed in
a later experimental study by~\cite{genuer:2010}.  From a theoretical
point of view, \cite{ishwaran:2007} provides a detailed theoretical
development of a simplified version of MDA, giving key insights for the
understanding of the actual MDA.

\section{Theoretical study}
\label{sec:6:theory}

Let us now consider the MDI  importance
as defined by Equation~\ref{eq:mdi}, and let us assume a set $V= \{X_1, ..., X_p\}$  of {\em categorical}
input variables and a {\em categorical} output $Y$. For the sake of
simplicity we will  use the Shannon entropy as impurity measure, and focus on
totally randomized trees; later on we will discuss other impurity measures and tree construction algorithms.

Given a training sample ${\cal L}$ of $N$ joint observations of $X_1, ..., X_p,
Y$ independently drawn from the joint distribution $P(X_1, ..., X_p, Y)$, let us
assume that we infer from it an infinitely large ensemble of \textit{totally
randomized and fully developed trees}. In this setting, a totally randomized and
fully developed tree is defined as a decision tree in which each node $t$ is
partitioned using a variable $X_j$ picked uniformly at random among those not
yet used at the parent nodes of $t$, and where each $t$ is split into $|{\cal
X}_j|$ sub-trees, i.e., one for each possible value of ${\cal X}_j$, and where
the recursive construction process halts only when all $p$ variables have been
used along the current branch.  Hence, in such a tree, leaves are all at the
same depth $p$, and the set of leaves of a fully developed tree is in bijection
with the set $\cal X$ of all possible joint configurations of the $p$ input
variables. For example, if the input variables are all binary, the resulting
tree will have exactly $2^{p}$ leaves.

\begin{theorem}\label{thm:imp}
The MDI importance of $X_j \in V$ for $Y$ as computed
with an   infinite ensemble of fully developed totally randomized trees and an
infinitely large training sample is:
  \begin{equation}\label{eqn:imp-full}
  \text{Imp}(X_j)=\sum_{k=0}^{p-1} \frac{1}{C_p^k} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B),
  \end{equation}
\end{theorem}
\noindent where $V^{-j}$ denotes the subset $V \setminus \{X_j\}$, ${\cal
P}_k(V^{-j})$ is the set of subsets of  $V^{-j}$ of cardinality $k$, and
$I(X_j;Y|B)$ is the conditional mutual information of $X_{j}$ and $Y$ given the
variables in $B$.

\begin{proof}
\todo{}
\end{proof}

\begin{theorem}\label{thm:sum-of-imp}
For any ensemble of fully developed trees in asymptotic learning sample size
conditions (e.g., in the same conditions as those of Theorem~\ref{thm:imp}), we
have that
\begin{equation}\label{eqn:sum-of-imp}
\sum_{j=1}^{p} \text{Imp}(X_j) = I(X_{1}, \ldots, X_{p} ; Y).
\end{equation}
\end{theorem}

\begin{proof}
\todo{}
\end{proof}

Together, theorems \ref{thm:imp} and \ref{thm:sum-of-imp} show that  variable
importances derived from totally randomized trees in asymptotic conditions
provide a three-level decomposition of the information $I(X_{1}, \ldots, X_{p}
; Y)$ contained in the set of input variables about the output variable. The
first level is a decomposition among input variables (see Equation~\ref{eqn
:sum-of-imp} of Theorem~\ref{thm:sum-of-imp}),  the second level is a
decomposition along the degrees $k$ of interaction terms of a variable with the
other ones (see the outer sum in Equation~\ref{eqn:imp-full} of
Theorem~\ref{thm:imp}), and the third level is a decomposition along the
combinations $B$ of interaction terms of fixed size $k$ of possible interacting
variables (see the inner sum in Equation~\ref{eqn:imp-full}).

We observe that the decomposition includes, for each variable, each and every
interaction term of each and every degree weighted in a fashion resulting only
from the combinatorics of possible interaction terms. In particular, since all
$I(X_j;Y|B)$ terms are at most equal to $H(Y)$, the prior entropy of $Y$,  the
$p$ terms of the outer sum of Equation~\ref{eqn:imp-full} are each upper
bounded by
\begin{equation}
\frac{1}{C^k_p}\frac{1}{p-k}\sum_{B \in {\cal P}_k(V^{-j})}
H(Y)=\frac{1}{C^k_p} \frac{1}{p-k} {C^k_{p-1}} H(Y) = \frac{1}{p}H(Y).
\end{equation}
As such,
the second level decomposition resulting from totally randomized trees makes the
$p$ sub-importance terms
\begin{equation}
\frac{1}{C^k_p} \frac{1}{p-k} \sum_{B \in {\cal P}_k(V^{-j})} I(X_j;Y|B)
\end{equation}
to equally contribute (at most) to the total
importance, even though they each include a combinatorially different number of
terms.


\section{Relevance of variables}
\label{sec:6:variable-relevance}


\section{Variable importances in random forest variants}
\label{sec:6:variants}


\section{Illustration}
\label{sec:6:illustration}

