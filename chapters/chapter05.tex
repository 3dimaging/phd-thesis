\chapter{Computational efficiency}\label{ch:complexity}

\begin{remark}{Outline}
In this chapter, we study the computational efficiency of tree-based ensemble
methods. In Section~\ref{sec:5:complexity}, we first derive both time and space
complexity of the induction algorithm, as presented in chapters~\ref{ch:cart}
and \ref{ch:forest}, for several randomized variants of the algorithm. In
Section~\ref{sec:5:impl}, we then discuss technical details that are critical
for efficiently  implementing random forests. Finally, we benchmark in
Section~\ref{sec:5:benchmarks} the random forest implementation developed
within work and compare our solution with alternative implementations.
\end{remark}

\section{Complexity analysis}
\label{sec:5:complexity}

% foreword sur l'hypothèse des splits
% Secondary goal of minizizing time complexity among trees of same accuracy

% * Overall complexity
%     - Best cases => Master theorem and/or Akra Bazzi method
%         RF: O(n log^2 n)
%         ETs: O(n log n)
%     - Worst cases
%         RF: O(log(H(n)))
%         ETs: O(n^2)
%     - Average case => Should still be good (see Quicksort average complexity)
%       http://en.wikipedia.org/wiki/Quicksort
%       Solve: T(n) = nlog(n) + 1/(n-1) \sum_{i=1}^{n-1} [T(i) + T(n-i)]
%       idem pour ETs
%     - Assume that splits are at worse rho / (1 - rho)


% Average Time Complexity of Decision Trees (Intelligent Systems ...
% ftp://icksie.no-ip.org/.../Average%20time%20complexity%20of%20decis...‎
% by I Chikalov - ‎Cited by 2 - ‎Related articles
% average depth

% The Time Complexity of Decision Tree Induction - CiteSeer
% citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.2193...‎
% by JK Martin - ‎1995 - ‎Cited by 19 - ‎Related articles
% analyse en fonction de la hauteur, on utilise qu'une fois un attribut par branche (multi-way split)

% High Computational Complexity of the Decision Tree Induction with ...
% www.mimuw.edu.pl/~rlatkows/.../latkowski2003complexity.pdf‎
% by R l Latkowski - ‎Cited by 1 - ‎Related articles

% A Fast Decision Tree Learning Algorithm
% pdf.aminer.org/000/014/.../a_fast_decision_tree_learning_algorithm.pdf‎
% by J Su - ‎2006 - ‎Cited by 39 - ‎Related articles
% c4.5, multi-way splits

% words on big-o notations


\section{Implementation}
\label{sec:5:impl}

% foreword on scikit-learn

\subsection{Data structures}

% growing vector vs object oriented

\subsection{Building decision trees}

% découpage criterion/splitter/builder
% growing vector
% replace stack with priority queue

\subsection{Splitting nodes}

%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling

\subsection{Shared computations}

%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading

\subsection{Sparse data}


\section{Benchmarks}
\label{sec:5:benchmarks}

% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle





% \todo{Rewrite later...}

% \section{Data structures}

% Implementing decision trees involves many issues that are easily overlooked if
% not considered with care. The first of these issues concerns the choice of the
% data structure for representing decision trees.

% Among all possible ways of representing a tree, one of the simplest and most
% direct representation is to adopt an object-oriented approach. In this
% paradigm, a decision tree is naturally represented as a hierarchy of high-level
% objects, where each object corresponds to a node of the tree and comprises
% attributes referencing its children or storing its split and value. Such a
% representation would make for a correct, intuitive and flexible implementation
% but may in fact not be the most appropriate when aiming for high-performance.
% One of the biggest issues indeed is that object-oriented programming usually
% fragments complex and nested objects into non-contiguous memory blocks, thereby
% requiring multiple levels of indirection for traversing the structure. In
% particular, this design can easily impair computing times in performance-
% critical applications, e.g., by not making it possible to leverage CPU cache or
% pre- fetching mechanisms.

% At the price of less abstraction and flexibility, we adopt instead in this work
% a low-level representation of decision trees, allowing us for a fine-grained
% and complete control over memory management and CPU mechanisms. The tree
% representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
