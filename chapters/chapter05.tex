\chapter{Computational efficiency}\label{ch:complexity}

\begin{remark}{Outline}
In this chapter, we study the computational efficiency of tree-based ensemble
methods. In sections~\ref{sec:5:time} and \ref{sec:5:space}, we first derive both time and space
complexity of decision trees and random forests, as presented in chapters~\ref{ch:cart}
and \ref{ch:forest} for several randomized variants. In
Section~\ref{sec:5:impl}, we then discuss technical details that are critical
for efficiently  implementing random forests. Finally, we benchmark in
Section~\ref{sec:5:benchmarks} the random forest implementation developed
within this work and compare our solution with alternative implementations.
\end{remark}

\section{Time complexity}
\label{sec:5:time}

The dominant objective of most machine learning methods is to find models that
maximize accuracy. For models of equivalent performance however, a
secondary objective is usually to minimize complexity. In this work, we
distinguish between two definitions of complexity: \textit{time complexity} and
\textit{space complexity}. Time complexity corresponds to the time, or
equivalently to the number of operations, required for building a model. The
lower the time complexity, the shorter it takes for building models. By contrast, space
complexity measures the space required for representing the model. In the case
of decision trees, space complexity can be defined, e.g., as the average number
of nodes or as the average depth of the leaves. All other things being equal, the
lower the space complexity, usually the more interpretable the model. In this
section, we first study the time complexity of random forests.
Section~\ref{sec:5:space} then outlines results regarding the space
complexity of decision trees.

Let $T(N)$ denotes the time complexity for building a decision tree from a
learning set ${\cal L}$ of $N$ samples. From Algorithm~\ref{algo:induction},
$T(N)$ corresponds to the number of operations required for finding a split $s$, partitioning the
node samples into ${\cal L}_{t_L}$ and ${\cal L}_{t_R}$ and then recursively
building two sub-trees respectively from $N_{t_L}$ and $N_{t_R}$ samples. Without
loss of generality, let us assume that the learning samples all have distinct
input values, such that it is possible to build a fully developed decision tree
where each leaf contains exactly one sample. Under this assumption, determining
time complexity therefore amounts to solve recurrence equation
\begin{equation}
\begin{cases}
T(1) = \BigO{1} \\
T(N) = c(N) + T(N_{t_L}) + T(N_{t_R}),
\end{cases}
\end{equation}
where $c(N)$ is the time complexity for finding a split $s$ and partitioning
the node samples.

% foreword sur l'hypothèse des splits => en plys d'etre bonne niveau accuracy (cf consistence), ca permet une recherche de split qui est efficace


\begin{remark}{Big $\BigO{.}$ notation}
% https://en.wikipedia.org/wiki/Big_O_notation
\end{remark}

% Average Time Complexity of Decision Trees (Intelligent Systems ...
% ftp://icksie.no-ip.org/.../Average%20time%20complexity%20of%20decis...‎
% by I Chikalov - ‎Cited by 2 - ‎Related articles
% average depth

% The Time Complexity of Decision Tree Induction - CiteSeer
% citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.2193...‎
% by JK Martin - ‎1995 - ‎Cited by 19 - ‎Related articles
% analyse en fonction de la hauteur, on utilise qu'une fois un attribut par branche (multi-way split)

% High Computational Complexity of the Decision Tree Induction with ...
% www.mimuw.edu.pl/~rlatkows/.../latkowski2003complexity.pdf‎
% by R l Latkowski - ‎Cited by 1 - ‎Related articles

% A Fast Decision Tree Learning Algorithm
% pdf.aminer.org/000/014/.../a_fast_decision_tree_learning_algorithm.pdf‎
% by J Su - ‎2006 - ‎Cited by 39 - ‎Related articles
% c4.5, multi-way splits

\section{Space complexity}
\label{sec:5:space}

% discuter en fonction du nombre de feuilles
%                      de la profondeur moyenne

\section{Implementation}
\label{sec:5:impl}

% foreword on scikit-learn

\subsection{Data structures}

% growing vector vs object oriented

\subsection{Building decision trees}

% découpage criterion/splitter/builder
% growing vector
% replace stack with priority queue

\subsection{Splitting nodes}

%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling

\subsection{Shared computations}

%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading

\subsection{Sparse data}


\section{Benchmarks}
\label{sec:5:benchmarks}

% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle





% \todo{Rewrite later...}

% \section{Data structures}

% Implementing decision trees involves many issues that are easily overlooked if
% not considered with care. The first of these issues concerns the choice of the
% data structure for representing decision trees.

% Among all possible ways of representing a tree, one of the simplest and most
% direct representation is to adopt an object-oriented approach. In this
% paradigm, a decision tree is naturally represented as a hierarchy of high-level
% objects, where each object corresponds to a node of the tree and comprises
% attributes referencing its children or storing its split and value. Such a
% representation would make for a correct, intuitive and flexible implementation
% but may in fact not be the most appropriate when aiming for high-performance.
% One of the biggest issues indeed is that object-oriented programming usually
% fragments complex and nested objects into non-contiguous memory blocks, thereby
% requiring multiple levels of indirection for traversing the structure. In
% particular, this design can easily impair computing times in performance-
% critical applications, e.g., by not making it possible to leverage CPU cache or
% pre- fetching mechanisms.

% At the price of less abstraction and flexibility, we adopt instead in this work
% a low-level representation of decision trees, allowing us for a fine-grained
% and complete control over memory management and CPU mechanisms. The tree
% representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
