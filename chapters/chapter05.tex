\chapter{Computational efficiency}\label{ch:complexity}

\begin{remark}{Outline}
In this chapter, we study the computational efficiency of tree-based ensemble
methods. In sections~\ref{sec:5:complexity-fit} and \ref{sec:5:complexity-predict},
we first derive and discuss the time complexity for building and
predicting from decision trees and random forests, as presented in
chapters~\ref{ch:cart} and \ref{ch:forest}. In Section~\ref{sec:5:impl}, we
then discuss technical details that are critical for efficiently  implementing
random forests. Finally, we benchmark in Section~\ref{sec:5:benchmarks} the
random forest implementation developed within this work and compare our
solution with alternative implementations.
\end{remark}

\section{Complexity of building random forests}
\label{sec:5:complexity-fit}

The dominant objective of most machine learning methods is to find models that
maximize accuracy. For models of equivalent performance however, a secondary
objective is usually to minimize complexity, given some definition of the
concept of complexity. A first and immediate facet of complexity in decision
trees and random forests is the time complexity for learning models, that is
the number of operations required for building models from data.

Given the exponential growth in the number of possible partitions of $N$
samples, we chose in Section~\ref{sec:3:splitting-rules} to restrict splitting
rules to binary splits defined on single variables. Not only this is sufficient
for reaching good accuracy (as discussed in Section~\ref{sec:4:consistency}),
it also allows for time complexity to effectively remain within reasonable bounds.

Formally, let $T(N)$ denotes the time complexity for building a decision tree from a
learning set ${\cal L}$ of $N$ samples. From Algorithm~\ref{algo:induction},
$T(N)$ corresponds to the number of operations required for finding a split $s$, partitioning the
node samples into ${\cal L}_{t_L}$ and ${\cal L}_{t_R}$ and then recursively
building two sub-trees respectively from $N_{t_L}$ and $N_{t_R}$ samples. Without
loss of generality, let us assume that the learning samples all have distinct
input values, such that it is possible to build a fully developed decision tree
where each leaf contains exactly one sample. Under this assumption, determining
time complexity therefore amounts to solve recurrence equation
\begin{equation}
\begin{cases}
T(1) = \BigO{1} \\
T(N) = c(N) + T(N_{t_L}) + T(N_{t_R}),
\end{cases}
\end{equation}
where $c(N)$ is the time complexity for finding a split $s$ and partitioning
the node samples.

\begin{remark}{Big O notations}
Time complexity analyzes the asymptotic behavior of an algorithm
with respect to the size of its input and its hyper-parameters. In this way,
big O notations are used to formally express terms that are asymptotically dominant
with respect to the number of steps in the algorithm, discarding the smaller terms that can be neglected. Formally,
we write that...
% https://en.wikipedia.org/wiki/Big_O_notation
\end{remark}

% Average Time Complexity of Decision Trees (Intelligent Systems ...
% ftp://icksie.no-ip.org/.../Average%20time%20complexity%20of%20decis...‎
% by I Chikalov - ‎Cited by 2 - ‎Related articles
% average depth

% The Time Complexity of Decision Tree Induction - CiteSeer
% citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.2193...‎
% by JK Martin - ‎1995 - ‎Cited by 19 - ‎Related articles
% analyse en fonction de la hauteur, on utilise qu'une fois un attribut par branche (multi-way split)

% High Computational Complexity of the Decision Tree Induction with ...
% www.mimuw.edu.pl/~rlatkows/.../latkowski2003complexity.pdf‎
% by R l Latkowski - ‎Cited by 1 - ‎Related articles

% A Fast Decision Tree Learning Algorithm
% pdf.aminer.org/000/014/.../a_fast_decision_tree_learning_algorithm.pdf‎
% by J Su - ‎2006 - ‎Cited by 39 - ‎Related articles
% c4.5, multi-way splits

\section{Complexity of making predictions}
\label{sec:5:complexity-predict}

% discuter en fonction du nombre de feuilles
%                      de la profondeur moyenne

\section{Implementation}
\label{sec:5:impl}

% foreword on scikit-learn

\subsection{Data structures}

% growing vector vs object oriented

\subsection{Building decision trees}

% découpage criterion/splitter/builder
% growing vector
% replace stack with priority queue

\subsection{Splitting nodes}

%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling

\subsection{Shared computations}

%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading

\subsection{Sparse data}


\section{Benchmarks}
\label{sec:5:benchmarks}

% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle





% \todo{Rewrite later...}

% \section{Data structures}

% Implementing decision trees involves many issues that are easily overlooked if
% not considered with care. The first of these issues concerns the choice of the
% data structure for representing decision trees.

% Among all possible ways of representing a tree, one of the simplest and most
% direct representation is to adopt an object-oriented approach. In this
% paradigm, a decision tree is naturally represented as a hierarchy of high-level
% objects, where each object corresponds to a node of the tree and comprises
% attributes referencing its children or storing its split and value. Such a
% representation would make for a correct, intuitive and flexible implementation
% but may in fact not be the most appropriate when aiming for high-performance.
% One of the biggest issues indeed is that object-oriented programming usually
% fragments complex and nested objects into non-contiguous memory blocks, thereby
% requiring multiple levels of indirection for traversing the structure. In
% particular, this design can easily impair computing times in performance-
% critical applications, e.g., by not making it possible to leverage CPU cache or
% pre- fetching mechanisms.

% At the price of less abstraction and flexibility, we adopt instead in this work
% a low-level representation of decision trees, allowing us for a fine-grained
% and complete control over memory management and CPU mechanisms. The tree
% representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
