\chapter{Computational efficiency}\label{ch:complexity}

\begin{remark}{Outline}
In this chapter, we study the computational efficiency of tree-based ensemble
methods. In sections~\ref{sec:5:complexity-fit} and \ref{sec:5:complexity-predict},
we derive and discuss the time complexity of random forests, first for building them from data and then for making predictions. In
Section~\ref{sec:5:impl}, we discuss technical details that are critical
for efficiently  implementing random forests. Finally, we conclude in
Section~\ref{sec:5:benchmarks} with benchmarks of the random forest implementation developed
within this work and compare our solution with alternative implementations.
\end{remark}

\section{Complexity of the induction procedure}
\label{sec:5:complexity-fit}

The dominant objective of most machine learning methods is to find models that
maximize accuracy. For models of equivalent performance, a secondary objective
is usually to minimize complexity. Complexity, however, has many facets. A
first and immediate notion of complexity in decision trees and random forests is
the \textit{time complexity} for learning models, that is the number of
operations required for building models from data.

Given the exponential growth in the number of possible partitions of $N$
samples, we chose in Section~\ref{sec:3:splitting-rules} to restrict splitting
rules to binary splits defined on single variables. Not only this is sufficient
for reaching good accuracy (as discussed in Section~\ref{sec:4:consistency}),
it also allows for time complexity to effectively remain within reasonable bounds.

Formally, let $T(N)$ denotes the time complexity for building a decision tree from a
learning set ${\cal L}$ of $N$ samples. From Algorithm~\ref{algo:induction},
$T(N)$ corresponds to the number of operations required for splitting a node of $N$ samples and then for recursively
building two sub-trees respectively from $N_{t_L}$ and $N_{t_R}$ samples. Without
loss of generality, let us assume that the learning samples all have distinct
input values, such that it is possible to build a fully developed decision tree
where each leaf contains exactly one sample. Under this assumption, determining
time complexity therefore amounts to solve the recurrence equation
\begin{equation}\label{eqn:complexity:rec}
\begin{cases}
T(1) = O(1) \\
T(N) = c(N) + T(N_{t_L}) + T(N_{t_R}),
\end{cases}
\end{equation}
where $c(N)$ is the time complexity for splitting a node of $N$ samples. For
$N=1$, the node is necessarily pure, hence the constant time complexity $O(1)$.
For $N>1$, $c(N)$ corresponds to the time complexity for finding a split $s$
and then partitioning the node samples into ${t_L}$ and ${t_R}$. This later
operation requires at least to iterate over all $N$ samples, which sets a
linear lower bound on time complexity within a node, i.e., $c(N)=\Omega(N)$. As
for finding the split $s$, this can be achieved in several ways, as outlined in
the previous chapters  for several randomized variants of the induction
procedure. As we will see, not only this is an impact on the accuracy of the
resulting model, it also drives the time complexity of the induction procedure.

\begin{remark}{Big O notations}
Time complexity analyzes the asymptotic behavior of an algorithm
with respect to the size of its input and its hyper-parameters. In this way,
big O notations are used to formally express an asymptotic upper bound on the
growth rate of the number of steps in the algorithm. Formally,
we write that
\begin{equation}
f(n) =  O(g(n)) \quad\text{if}\quad \exists c > 0, \exists n_0 > 0, \forall n > n_0, f(n) \leq c g(n),
\end{equation}
to express that  $f(n)$ is asymptotically upper bounded by $g(n)$, up to some neglectable constant factor $c$.
Similarly, big $\Omega$ notations are used to express an asymptotic lower
bound on the growth rate of the number of steps in the algorithm. Formally,
we write that
\begin{equation}
f(n) =  \Omega(g(n)) \quad\text{if}\quad \exists c > 0, \exists n_0 > 0, \forall n > n_0,  c g(n) \leq f(n),
\end{equation}
to express that $f(n)$ is asymptoptically lower bounded by $g(n)$.
Consequently, if $f(n)$ is both $O(g(n))$ and $\Omega(g(n))$ then $f(n)$ is
both lower bounded and upper bounded asymptotically by $g(n)$ (possibly for different constants),
which we write using big $\Theta$ notations. That is,
\begin{equation}
f(n) = \Theta(g(n)) \quad\text{if}\quad  f(n) =  O(g(n)) = \Omega(g(n)).
\end{equation}
\end{remark}

In the original CART induction procedure~\citep{breiman:1984}
(Algorithms~\ref{algo:findsplit} and \ref{algo:findsplit:x_j}), finding a split
$s$  requires, for each of the $p$ variables $X_j$ (for $j=1,\dots,p$), to sort
the values $x_{i,j}$ of all $N$ node samples (for $i=1,\dots,N$) and then to
iterate over these in order to find the best threshold $v$. The most costly
operation is sorting, whose time complexity is at worst $O(N \log N)$. As a
result, the overall within-node complexity is $c(N) = (p N \log N)$. In a
randomized tree as built with the Random Forest algorithm~\citep{breiman:2001}
(RF, Algorithm~\ref{algo:findsplit:random}), the search of the best split $s$
is carried out in the same way, but only on $K \leq p$ of the input variables,
resulting in a within-node complexity $c(N) = O(K N \log N)$. In Extremely
Randomized Trees~\citep{geurts:2006} (ETs, Algorithm~\ref{algo:findsplit:et}),
discretization thresholds are drawn at random within the minimum and maximum
node sample values of $X_j$, making sort no longer necessary. As such, the
within-node complexity reduces to the time required for finding these lower and
upper bounds, which can be done in linear time, hence $c(N)=O(KN)$. Finally, in
Perfect Random Tree Ensembles~\citep{cutler:2001} (PERT,
Algorithm~\ref{algo:findsplit:pert}), cut-points $v$ are set midway between
two randomly drawn samples, which can be done in constant time, independently
of the number $N$ of node samples. Yet, the within-node complexity is lower
bounded by the time required for partitioning the node samples into ${t_L}$ and
${t_R}$, hence $c(N)=O(N)$.

Since both CART and PERT can respectively be considered as special cases of RF
and ETs with regards to time complexity (i.e., for $K=p$ in CART, for $K=1$ in
PERT), let us consider the overall complexity $T(n)$ when either
$c(N)=O(KN\log N)$ or $c(N)=O(KN)$ (for $K=1,\dots,p$). Time complexity
is studied in three cases:

\begin{itemize}
\item \textit{Best case.} The induction procedure is the most effective
      when node samples can always be partitioned into two balanced subsets of $\tfrac{N}{2}$
      samples.

\item \textit{Worst case.} By contrast, the induction procedure is the least
      effective when splits are unbalanced. In the worst case,
      this results in splits that move a single sample in the first sub-tree and
      all $N-1$ others in the second sub-tree.

\item \textit{Average case.} The average case corresponds to the average time
      complexity, as taken over all possible learning sets ${\cal L}$ and
      all random seeds $\theta$.  Since the analysis of this case is hardly
      feasible without strong assumptions on the structure of the problem,
      we consider instead the case where the sizes of the possible
      partitions of a node are all equiprobable.

\end{itemize}

\begin{remark}{Master Theorem}
Some of the results outlined in the remaining of this section make use of the
Master Theorem~\citep{goodrich:2006}, which recall below to be self-contained.

\begin{theorem}
Let consider the recurrence equation
\begin{equation}
\begin{cases}
T(1) = c & \text{if}\quad n < d\\
T(n) = aT(\frac{n}{b}) + f(n) & \text{if}\quad n \geq d
\end{cases}
\end{equation}
where $d \geq 1$ is an integer constant, $a > 0$, $c>0$ and $b>1$ are real constants, and
$f(n)$ is a function that is positive for $n \geq d$.

\begin{enumerate}
\item If there is a small constant $\epsilon > 0$, such that $f(n)$ is $O(n^{\log_b a - \epsilon})$, then $T(n)$ is $\Theta(n^{\log_b a})$.
\item If there is a constant $k \geq 0$, such that $f(n)$ is $O(n^{\log_b a} \log^k n)$, then $T(n)$ is $\Theta(n^{\log_b a} \log^{k+1} n)$.
\item If there are small constants $\epsilon > 0$ and $\delta < 1$, such that $f(n)$ is $\Omega(n^{\log_b a + \epsilon})$ and $af(\tfrac{n}{b}) \leq \delta f(n)$, for $n \geq d$, then $T(n)$ is $\Theta(f(n))$.
\end{enumerate}
\end{theorem}
\end{remark}

\begin{theorem}\label{thm:6:best:knlogn}
For $c(N)=O(K N\log N)$, the time complexity for building a decision
tree in the best case is $O(K N \log^2 N)$.
\end{theorem}

\begin{proof}
Since $O(K)=O(1)$, the dependency to $K$ in Equation~\ref{eqn:complexity:rec} can be factored out,
thereby rewriting $T(N)$ as $O(K) T'(N)$, where $T'(N)$ is
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N\log N) + 2 T'(\frac{N}{2})
\end{cases}
\end{equation}
in the best case. In this form, the second case of the Master Theorem applies for
$a=2$, $b=2$ and $k=1$. Accordingly, $T'(N)=O(N\log^{k+1} N)=O(N\log^2 N)$ and $T(N) = O(K N\log^2 N)$ in the best case.
\end{proof}

\begin{theorem}\label{thm:6:best:kn}
For $c(N)=O(K N)$, the time complexity for building a decision
tree in the best case is $O(K N \log N)$.
\end{theorem}

\begin{proof}
For $T(N) = O(K)T'(N)$, Equation~\ref{eqn:complexity:rec}
can be rewritten in the best case as
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N) + 2 T'(\frac{N}{2})
\end{cases}
\end{equation}
Again, the second case of the Master Theorem applies, this time for $a=2$, $b=2$ and $k=0$.
Accordingly, $T'(N)=O(N\log^{k+1} N)=O(N\log N)$ and $T(N) = O(K N\log N)$ in the case case.
\end{proof}

\begin{theorem}\label{thm:6:worst:knlogn}
For $c(N)=O(K N\log N)$, the time complexity for building a decision
tree in the worst case is $O(K N^2 \log N)$.
\end{theorem}

\begin{proof}
For $T(N) = O(K)T'(N)$, Equation~\ref{eqn:complexity:rec}
can be rewritten in the worst case as
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N\log N) +  T'(1) + T'(N-1)
\end{cases}
\end{equation}
It comes,
\begin{align}
T'(N) &= O(N\log N) + O(1) + T'(N-1) \nonumber \\
      &= O(1) + \sum_{i=2}^N O(1) + O(i\log i) \nonumber \\
      &= O(N) + \sum_{i=2}^N O(i\log i) \nonumber \\
      &\leq O(N) + O(\log N) \sum_{i=2}^N O(i) \nonumber \\
      &= O(N) + O(\log N) O(\frac{1}{2} (N-1)(N-2)) \nonumber \\
      &= O(N^2 \log N)
\end{align}
As a result, $T(N) = O(K N^2 \log N)$ in the worst case.
\end{proof}

\begin{theorem}\label{thm:6:worst:kn}
For $c(N)=O(K N)$, the time complexity for building a decision
tree in the worst case is $O(K N^2)$.
\end{theorem}

\begin{proof}
For $T(N) = O(K)T'(N)$, Equation~\ref{eqn:complexity:rec}
can be rewritten in the worst case as
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N) +  T'(1) + T'(N-1)
\end{cases}
\end{equation}
It comes,
\begin{align}
T'(N) &= O(N) + O(1) + T'(N-1) \nonumber \\
      &= O(1) + \sum_{i=2}^N O(1+i) \nonumber \\
      &= O(1) + O(\frac{1}{2} (N^2 + 3N - 4)) \nonumber \\
      &= O(N^2)
\end{align}
As a result, $T(N) = O(K N^2)$ in the worst case.
\end{proof}

\begin{theorem}\label{thm:6:average:knlogn}
For $c(N)=O(K N\log N)$, the time complexity for building a decision
tree in the average case is $O(K N \log^2 N)$.
\end{theorem}

\begin{proof}
For $T(N) = O(K)T'(N)$, Equation~\ref{eqn:complexity:rec}
can be rewritten in the average case as
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N\log N) +  \frac{1}{N-1} \sum_{i=1}^{N-1} T'(i) + T'(N-i).
\end{cases}
\end{equation}
By symmetry and by multiplying both sides of the last equation by $(N-1)$, it comes
\begin{equation}\label{eqn:6:average:knlogn:1}
(N-1) T'(N) = O(N\log N)(N-1) +  2 \sum_{i=1}^{N-1} T'(i).
\end{equation}
For $N \geq 3$, we similarly have
\begin{equation}\label{eqn:6:average:knlogn:2}
(N-2) T'(N-1) = O((N-1) \log(N-1))(N-2) +  2 \sum_{i=1}^{N-2} T'(i).
\end{equation}
Subtracting Equation~\ref{eqn:6:average:knlogn:2} from \ref{eqn:6:average:knlogn:1},
it comes after simplifications and division of both sides by $N(N-1)$:
\begin{equation}
\frac{T'(N)}{N} = \frac{T'(N-1)}{N-1} + O(\frac{2}{N} \log(N-1) + \log \frac{N}{N-1} ).
\end{equation}
Let us introduce $S(N) = \frac{T'(N)}{N}$. From the last equation, it then comes
\begin{align}
S(N) &= S(N-1) + O(\frac{2}{N} \log(N-1) + \log \frac{N}{N-1} ) \nonumber \\
     &= O(1) + O(\sum_{i=2}^N \frac{2}{i} \log(i-1) + \log \frac{i}{i-1} ) \nonumber \\
     &= O(1) + O(\log N) + O(2 \sum_{i=2}^N \frac{1}{i} \log(i-1) ) \nonumber \\
     &\leq O(1) + O(\log N) + O(2 \log N) O(\sum_{i=2}^N \frac{1}{i} ) \nonumber \\
     &= O(1) + O(\log N) + O(2 \log N) O(H_N - 1) \nonumber \\
     &= O(H_N \log N)
\end{align}
where $H_N$ is the $N$-th harmonic number. Using
\begin{equation}
H_N \approx \log N + \gamma + \frac{1}{2N} + O(\frac{1}{N^2})
\end{equation}
as approximation (where $\gamma$ is the Euler-Mascheroni constant), we have
\begin{align*}
T'(N) &= O(N H_N \log N) \nonumber \\
      &= O(N (\log N + \gamma + \frac{1}{2N} + \frac{1}{N^2}) \log N  ) \nonumber \\
    &= O(N \log^2 N).
\end{align*}
As a result, $T(N) = O(KN \log^2 N)$ in the average case.
\end{proof}

\begin{theorem}\label{thm:6:average:kn}
For $c(N)=O(K N)$, the time complexity for building a decision
tree in the average case is $O(K N \log N)$.
\end{theorem}

\begin{proof}
For $T(N) = O(K)T'(N)$, Equation~\ref{eqn:complexity:rec}
can be rewritten in the average case as
\begin{equation}
\begin{cases}
T'(1) = O(1) \\
T'(N) = O(N) +  \frac{1}{N-1} \sum_{i=1}^{N-1} T'(i) + T'(N-i).
\end{cases}
\end{equation}
By symmetry and by multiplying both sides of the last equation by $(N-1)$, it comes
\begin{equation}\label{eqn:6:average:kn:1}
(N-1) T'(N) = O(N)(N-1) +  2 \sum_{i=1}^{N-1} T'(i).
\end{equation}
For $N \geq 3$, we similarly have
\begin{equation}\label{eqn:6:average:kn:2}
(N-2) T'(N-1) = O(N-1)(N-2) +  2 \sum_{i=1}^{N-2} T'(i).
\end{equation}
Subtracting Equation~\ref{eqn:6:average:kn:2} from \ref{eqn:6:average:kn:1},
it comes after simplifications and division of both sides by $N(N-1)$:
\begin{equation}
\frac{T'(N)}{N} = \frac{T'(N-1)}{N-1} + O(\frac{2}{N}).
\end{equation}
Let us introduce $S(N) = \frac{T'(N)}{N}$. From the last equation, it then comes
\begin{align}
S(N) &= S(N-1) + O(\frac{2}{N}) \nonumber \\
     &= O(2 \sum_{i=1}^N \frac{1}{i} ) \nonumber \\
     &= O(2 H_N)
\end{align}
where $H_N$ is the $N$-th harmonic number. Using
\begin{equation}
H_N \approx \log N + \gamma + \frac{1}{2N} + O(\frac{1}{N^2})
\end{equation}
as approximation (where $\gamma$ is the Euler-Mascheroni constant), we have
\begin{align}
T'(N) &= O(2 H_N N) \nonumber \\
&= O(2 N \log N + 2 N\gamma + 1 + \frac{2}{N}) \nonumber \\
&= O(N \log N).
\end{align}
As a result, $T(N) = O(KN \log N)$ in the average case.
\end{proof}

From Theorems~\ref{thm:6:best:knlogn}-\ref{thm:6:average:kn}, the total
time complexity for building an ensemble of $M$ randomized trees can finally
be derived, as summarized in Table~\ref{table:complexity-fit}.

Building a single tree using the original CART algorithm requires between
$O(pN\log^2 N)$ and $O(pN^2\log N)$ depending on the problem.

\begin{table}
    \centering
    \begin{tabular}{| c | c c c |}
    \hline
         & \textit{Best case} & \textit{Worst case} & \textit{Average case}  \\
    \hline
    \hline
    CART & $O(pN\log^2 N)$ & $O(pN^2\log N)$ & $O(pN\log^2 N)$ \\
    Bagging & $O(Mp\widetilde{N}\log^2 \widetilde{N})$ & $O(Mp\widetilde{N}^2\log \widetilde{N})$ & $O(Mp\widetilde{N}\log^2 \widetilde{N})$  \\
    RF & $O(MK\widetilde{N}\log^2 \widetilde{N})$ & $O(MK\widetilde{N}^2\log \widetilde{N})$ & $O(MK\widetilde{N}\log^2 \widetilde{N})$  \\
    ETs & $O(MKN\log N)$ & $O(MKN^2)$ & $O(MKN\log N)$  \\
    PERT & $O(MN\log N)$ & $O(MN^2)$ & $O(MN\log N)$  \\
    \hline
    \end{tabular}
    \captionof{table}{Time complexity for building forests of $M$ randomized trees. $N$ denotes the number of samples in ${\cal L}$, $p$ the number of input variables and $K$ the number of variables randomly drawn at each node. $\widetilde{N} = 0.632 N$, due to the fact that bootstrap samples draw, on average, $63.2\%$ of unique samples.}
    \label{table:complexity-fit}
\end{table}

% Average Time Complexity of Decision Trees (Intelligent Systems ...
% ftp://icksie.no-ip.org/.../Average%20time%20complexity%20of%20decis...‎
% by I Chikalov - ‎Cited by 2 - ‎Related articles
% average depth

% The Time Complexity of Decision Tree Induction - CiteSeer
% citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.2193...‎
% by JK Martin - ‎1995 - ‎Cited by 19 - ‎Related articles
% analyse en fonction de la hauteur, on utilise qu'une fois un attribut par branche (multi-way split)

% High Computational Complexity of the Decision Tree Induction with ...
% www.mimuw.edu.pl/~rlatkows/.../latkowski2003complexity.pdf‎
% by R l Latkowski - ‎Cited by 1 - ‎Related articles

% A Fast Decision Tree Learning Algorithm
% pdf.aminer.org/000/014/.../a_fast_decision_tree_learning_algorithm.pdf‎
% by J Su - ‎2006 - ‎Cited by 39 - ‎Related articles
% c4.5, multi-way splits

\section{Complexity of the prediction procedure}
\label{sec:5:complexity-predict}

% discuter en fonction du nombre de feuilles
%                      de la profondeur moyenne

\section{Implementation}
\label{sec:5:impl}

\subsection{Scikit-Learn}

% foreword on scikit-learn

\subsection{Data structures}

% growing vector vs object oriented

\subsection{Building decision trees}

% découpage criterion/splitter/builder
% growing vector
% replace stack with priority queue
% support for sample_weight => this reduce complexity

\subsection{Splitting nodes}

%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling

\subsection{Shared computations}

%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading

% \subsection{Sparse data}


\section{Benchmarks}
\label{sec:5:benchmarks}

% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle





% \todo{Rewrite later...}

% \section{Data structures}

% Implementing decision trees involves many issues that are easily overlooked if
% not considered with care. The first of these issues concerns the choice of the
% data structure for representing decision trees.

% Among all possible ways of representing a tree, one of the simplest and most
% direct representation is to adopt an object-oriented approach. In this
% paradigm, a decision tree is naturally represented as a hierarchy of high-level
% objects, where each object corresponds to a node of the tree and comprises
% attributes referencing its children or storing its split and value. Such a
% representation would make for a correct, intuitive and flexible implementation
% but may in fact not be the most appropriate when aiming for high-performance.
% One of the biggest issues indeed is that object-oriented programming usually
% fragments complex and nested objects into non-contiguous memory blocks, thereby
% requiring multiple levels of indirection for traversing the structure. In
% particular, this design can easily impair computing times in performance-
% critical applications, e.g., by not making it possible to leverage CPU cache or
% pre- fetching mechanisms.

% At the price of less abstraction and flexibility, we adopt instead in this work
% a low-level representation of decision trees, allowing us for a fine-grained
% and complete control over memory management and CPU mechanisms. The tree
% representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
