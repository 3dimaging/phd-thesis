\chapter{Computational efficiency}\label{ch:complexity}

% * Implementation considerations
%     - Data structures
%     - Efficient implementation of criteria for iterative evaluation
%        + How to evaluate multiple candidate thresholds cheaply for
%          a given feature (algorithm, f-ordered)
%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling
%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading
% * Overall complexity
%     - Best cases => Master theorem and/or Akra Bazzi method
%         RF: O(n log^2 n)
%         ETs: O(n log n)
%     - Worst cases
%         RF: O(log(H(n)))
%         ETs: O(n^2)
%     - Average case => Should still be good (see Quicksort average complexity)
%       http://en.wikipedia.org/wiki/Quicksort
%       Solve: T(n) = nlog(n) + 1/(n-1) \sum_{i=1}^{n-1} [T(i) + T(n-i)]
% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle

\todo{Rewrite later...}

\section{Data structures}

Implementing decision trees involves many issues that are easily overlooked if
not considered with care. The first of these issues concerns the choice of the
data structure for representing decision trees.

Among all possible ways of representing a tree, one of the simplest and most
direct representation is to adopt an object-oriented approach. In this
paradigm, a decision tree is naturally represented as a hierarchy of high-level
objects, where each object corresponds to a node of the tree and comprises
attributes referencing its children or storing its split and value. Such a
representation would make for a correct, intuitive and flexible implementation
but may in fact not be the most appropriate when aiming for high-performance.
One of the biggest issues indeed is that object-oriented programming usually
fragments complex and nested objects into non-contiguous memory blocks, thereby
requiring multiple levels of indirection for traversing the structure. In
particular, this design can easily impair computing times in performance-
critical applications, e.g., by not making it possible to leverage CPU cache or
pre- fetching mechanisms.

At the price of less abstraction and flexibility, we adopt instead in this work
a low-level representation of decision trees, allowing us for a fine-grained
and complete control over memory management and CPU mechanisms. The tree
representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
