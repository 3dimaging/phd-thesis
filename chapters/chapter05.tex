\chapter{Computational efficiency}\label{ch:complexity}

\begin{remark}{Outline}
In this chapter, we study the computational efficiency of tree-based ensemble
methods. In sections~\ref{sec:5:complexity-fit} and \ref{sec:5:complexity-predict},
we derive and discuss the time complexity of random forests, first for building them from data and then for making predictions. In
Section~\ref{sec:5:impl}, we discuss technical details that are critical
for efficiently  implementing random forests. Finally, we conclude in
Section~\ref{sec:5:benchmarks} with benchmarks of the random forest implementation developed
within this work and compare our solution with alternative implementations.
\end{remark}

\section{Complexity of building random forests}
\label{sec:5:complexity-fit}

The dominant objective of most machine learning methods is to find models that
maximize accuracy. For models of equivalent performance, a secondary objective
is usually to minimize complexity. Complexity, however, has many facets. A
first and immediate notion of complexity in decision trees and random forests is
the \textit{time complexity} for learning models, that is the number of
operations required for building models from data.

Given the exponential growth in the number of possible partitions of $N$
samples, we chose in Section~\ref{sec:3:splitting-rules} to restrict splitting
rules to binary splits defined on single variables. Not only this is sufficient
for reaching good accuracy (as discussed in Section~\ref{sec:4:consistency}),
it also allows for time complexity to effectively remain within reasonable bounds.

Formally, let $T(N)$ denotes the time complexity for building a decision tree from a
learning set ${\cal L}$ of $N$ samples. From Algorithm~\ref{algo:induction},
$T(N)$ corresponds to the number of operations required for splitting a node of $N$ samples and then recursively
building two sub-trees respectively from $N_{t_L}$ and $N_{t_R}$ samples. Without
loss of generality, let us assume that the learning samples all have distinct
input values, such that it is possible to build a fully developed decision tree
where each leaf contains exactly one sample. Under this assumption, determining
time complexity therefore amounts to solve the recurrence equation
\begin{equation}
\begin{cases}
T(1) = O(1) \\
T(N) = c(N) + T(N_{t_L}) + T(N_{t_R}),
\end{cases}
\end{equation}
where $c(N)$ is the time complexity for splitting a node of $N$ samples. For
$N=1$, the node is necessarily pure, hence the constant time complexity
$O(1)$. For $N>1$, $c(N)$ corresponds to the time complexity for finding a
split $s$ and then partitioning the node samples into $N_{t_L}$ and $N_{t_R}$.
Let us already note that this later operation requires at least to iterate over
all $N$ samples, which sets a linear lower bound on time complexity within a
node, i.e., $c(N)=\Omega(N)$. As previously outlined in
algorithms~\ref{algo:findsplit}, \ref{algo:findsplit:x_j},
\ref{algo:findsplit:random}, \ref{algo:findsplit:pert} and
\ref{algo:findsplit:et}, random forest algorithms differ from each other
in the way they find the split $s$ used to partition the node samples.
Not only this is an impact on the accuracy of the resulting model, it also
drives the time complexity for building the model.

\begin{remark}{Big O notations}
Time complexity analyzes the asymptotic behavior of an algorithm
with respect to the size of its input and its hyper-parameters. In this way,
big O notations are used to formally express an asymptotic upper bound on the
growth rate of the number of steps in the algorithm. Formally,
we write that
\begin{equation}
f(n) =  O(g(n)) \quad\text{if}\quad \exists c > 0, \exists n_0 > 0, \forall n > n_0, f(n) \leq c g(n).
\end{equation}
Similarly, big $\Omega$ notations are used to express an asymptotic lower
bound on the growth rate of the number of steps in the algorithm. Formally,
we write that
\begin{equation}
f(n) =  \Omega(g(n)) \quad\text{if}\quad \exists c > 0, \exists n_0 > 0, \forall n > n_0,  c g(n) \leq f(n).
\end{equation}
Consequently, if $f(n)$ is both $O(g(n))$ and $\Omega(g(n))$ then $f(n)$ is
both lower bounded and upper bounded asymptotically by $g(n)$ (possibly for different constants),
which we write using big $\Theta$ notations. That is,
\begin{equation}
f(n) = \Theta(g(n)) \quad\text{if}\quad  f(n) =  O(g(n)) = \Omega(g(n)).
\end{equation}
\end{remark}

In PERT, finding a split $s$ ...

% Average Time Complexity of Decision Trees (Intelligent Systems ...
% ftp://icksie.no-ip.org/.../Average%20time%20complexity%20of%20decis...‎
% by I Chikalov - ‎Cited by 2 - ‎Related articles
% average depth

% The Time Complexity of Decision Tree Induction - CiteSeer
% citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.2193...‎
% by JK Martin - ‎1995 - ‎Cited by 19 - ‎Related articles
% analyse en fonction de la hauteur, on utilise qu'une fois un attribut par branche (multi-way split)

% High Computational Complexity of the Decision Tree Induction with ...
% www.mimuw.edu.pl/~rlatkows/.../latkowski2003complexity.pdf‎
% by R l Latkowski - ‎Cited by 1 - ‎Related articles

% A Fast Decision Tree Learning Algorithm
% pdf.aminer.org/000/014/.../a_fast_decision_tree_learning_algorithm.pdf‎
% by J Su - ‎2006 - ‎Cited by 39 - ‎Related articles
% c4.5, multi-way splits

\section{Complexity of making predictions}
\label{sec:5:complexity-predict}

% discuter en fonction du nombre de feuilles
%                      de la profondeur moyenne

\section{Implementation}
\label{sec:5:impl}

% foreword on scikit-learn

\subsection{Data structures}

% growing vector vs object oriented

\subsection{Building decision trees}

% découpage criterion/splitter/builder
% growing vector
% replace stack with priority queue

\subsection{Splitting nodes}

%        + Sorting algorithms
%        + Approximately best splits
%             + Binning
%             + Subsampling

\subsection{Shared computations}

%     - Shared computations
%         > Splitters
%             - (Independent splitters)
%             - Pre-sorting + data reorganization (Breiman's strategy)
%             - Pre-sorting + sample_mask
%         > Multi-threading

% \subsection{Sparse data}


\section{Benchmarks}
\label{sec:5:benchmarks}

% * Comparison of existing implementations
%     - Experiments and/or implementation details

%         List existing implementations and mention in which complexity they
%         fall in (eg., in a Table).

%         Open-source
%         * C4.5
%         * CART
%         * TMVA !!
%         * Scikit-Learn
%         * Weka
%             + original
%             + fast-random-forest
%         * randomForest
%             + original
%             + PARF
%         * Breiman Fortran code
%         * party (R)
%         * Pierre
%         * OpenCV
%         * H2O
%         * clus (Leuven)

%         Closed source
%         * Sherwood (Criminisi)
%         * WiseRF
%         * Random jungle





% \todo{Rewrite later...}

% \section{Data structures}

% Implementing decision trees involves many issues that are easily overlooked if
% not considered with care. The first of these issues concerns the choice of the
% data structure for representing decision trees.

% Among all possible ways of representing a tree, one of the simplest and most
% direct representation is to adopt an object-oriented approach. In this
% paradigm, a decision tree is naturally represented as a hierarchy of high-level
% objects, where each object corresponds to a node of the tree and comprises
% attributes referencing its children or storing its split and value. Such a
% representation would make for a correct, intuitive and flexible implementation
% but may in fact not be the most appropriate when aiming for high-performance.
% One of the biggest issues indeed is that object-oriented programming usually
% fragments complex and nested objects into non-contiguous memory blocks, thereby
% requiring multiple levels of indirection for traversing the structure. In
% particular, this design can easily impair computing times in performance-
% critical applications, e.g., by not making it possible to leverage CPU cache or
% pre- fetching mechanisms.

% At the price of less abstraction and flexibility, we adopt instead in this work
% a low-level representation of decision trees, allowing us for a fine-grained
% and complete control over memory management and CPU mechanisms. The tree
% representation that we consider ...

%     > Data structures (arrays vs. object-oriented structures)
%       binary vs n-ary tree
%     > définir l'algo de construction
%     > definir l'algo de prédiction

%       * Recursive partition
%           Depth/Breadth/Best first
