\chapter{Background}\label{ch:background}

\section{Learning from data}

In the examples introduced in Chapter~\ref{ch:introduction}, the objective
which is sought is to find a systematic way of predicting a phenomenon given a
set of measurements. In machine learning terms, this goal is formulated as the
{\it supervised learning} task of infering from collected data a model that
predicts the value of an output variable based on the observed values of input
variables. As such, finding an appropriate model is based on the assumption
that the output variable does not take its value at random and that there
exists a relation between the inputs and the output. In medicine for instance,
the goal is to find a decision rule (i.e., a model) from a set of past cases
(i.e., the collected data) for predicting the condition of an incoming patient
(i.e., the output value) given a set of measurements such as age, sex, blood
pressure or history (i.e., the input values).

To give a more precise formulation, let us arrange the set of measurements on a
case in a pre-assigned order, i.e., take the input values to be $x_1, x_2, ...,
x_p$, where $x_j \in {\cal X}_j$ (for $j = 1, ..., p$) corresponds to the value
of the input variable $X_j$. Together, the input values $(x_1, x_2, ..., x_p)$
form a $p$-dimensional input vector $\mathbf{x}$ taking its values in ${\cal
X}_1 \times ... \times {\cal X}_p = {\cal X}$, where ${\cal X}$ is defined as
the input space. Similarly, let us define as $y \in {\cal Y}$ the value of the
output variable $Y$, where ${\cal Y}$ is defined as the output
space\footnote{Unless stated otherwise, supervised learning is reduced to the
prediction of a \textit{single} output variable. More generally however, this
framework can be defined as the prediction of one or several output variables.}.
By definition, both the input and the output spaces are assumed to respectively
contain all possible input vectors and all possible output values. Note that
input variables are sometimes known as {\it features}, input vectors as {\it
objects}, {\it examples} or {\it samples} and the output variable as {\it
target}.

Among variables that define the problem, we distinguish between two general
types. The former correspond to quantitative variables whose values are integer
or real numbers, such as age or blood pressure. The latter correspond to
qualitative variables whose values are symbolic, such as gender or condition.
Formally, we define them as follows:

\begin{definition}
A variable $X_j$ is \emph{numerical} or \emph{ordered} if ${\cal X}_j$ is a
totally ordered set.
\end{definition}

\begin{definition}
A variable $X_j$ is \emph{categorical} if ${\cal X}_j$ is a finite set of values,
without any natural order.
\end{definition}

In a typical supervised learning task, past observations are summarized by a
dataset called {\it learning set}. It consists in a set of observed input
vectors together with their actual output value and formally defined as
follows:

\begin{definition}
A \emph{learning set} ${\cal L}$ is a set of $N$
pairs of input vectors and output values $(\mathbf{x}_1, y_1), ...,
(\mathbf{x}_N, y_N)$, where $\mathbf{x}_i \in {\cal X}$ and $y_i \in {\cal Y}$.
\end{definition}

Equivalently, a set of $p$-input vectors $\mathbf{x}_i$ (for $i=1, ..., N$) can
be denoted by a $N\times p$ matrix $\mathbf{X}$, whose rows $i=1, ..., N$
correspond to input vectors $\mathbf{x}_i$ and columns $j=1, ..., p$ to input
variables $X_j$. Similarly, the corresponding output values can be written as a
vector $\mathbf{y}=(y_1, ..., y_N)$.

\begin{remark}{Data representation}
For optimal implementations of machine learning algorithms, data needs to
represented using structures which allows for high-performance numerical
computation. In this work, code snippets are described under  that assumption
that data is represented using a data structure similar to a NumPy
array~\citep{vanderwalt:2011}.

A NumPy array is basically a multidimensional uniform collection of values, all
of the same type and organized in a given shape. For instance, a matrix
$\mathbf{X}$ can be represented as a 2-dimensional NumPy array of shape $N
\times p$ that contains numbers (e.g., floating point values or integers). This
structure allows for random access in constant time, vectorized high-level
operations and efficient memory usage.

Additionally, using a data representation which is close to the matrix
formulation, like NumPy arrays, makes it possible to write implementations that
are close to their original textbook formulation, thereby making them easier to
understand and maintain. \end{remark}

In this framework, the supervised learning task can be stated as learning a
function $\varphi: {\cal X} \mapsto {\cal Y}$ from a learning set ${\cal
L}=(\mathbf{X}, \mathbf{y})$. In particular, the goal of the learning task is
to find a model such that its predictions $\varphi(\mathbf{x})$, also denoted
by the variable $\hat{Y}$, are as good as possible. If $Y$ is a categorical
variable then the learning task is a classification problem. If $Y$ is
numerical variable, then learning task is a regression problem. Without loss of
generality, the resulting models, or \textit{estimators}, can be defined as
follows:

\begin{definition}
A \emph{classifier} or \emph{classification rule} is a function $\varphi: {\cal X}
\mapsto {\cal Y}$, where ${\cal Y}$ is a finite set of classes denoted $\{1, 2, ..., J\}$.
\end{definition}

\begin{definition}
A \emph{regressor} is a function $\varphi: {\cal X} \mapsto {\cal Y}$, where ${\cal Y}=\mathbb{R}$.
\end{definition}

\begin{remark}{Estimator interface}
We follow in this work the API conventions proposed by~\citet{buitinck:2013}.
In particular, learning algorithms are described as estimators implementing the
following interface:

\begin{itemize}
\item[-] Hyper-parameters of an algorithm are all passed to the constructor of the
estimator. The constructor does not see any actual data. All it does
is to attach hyper-parameters values as public attributes to the estimator object.
\item[-] Learning is performed in a \texttt{fit} method. This method is called with
a learning set (e.g., supplied as two arrays \texttt{X\_train} and \texttt{y\_train}). Its
task is to run a learning algorithm and to determine model-specific parameters
from the data. The \texttt{fit} method always returns the estimator object
it was called on, which now serves as a model and can be used to make predictions.
\item[-] Predictions are performed through a \texttt{predict} method, taking
as input an array \texttt{X\_test} and producing as output the predictions for
\texttt{X\_test} based on the learned parameters. In the case of classification,
this method returns labels from ${\cal Y} = \{1, 2, ..., J\}$. In the case of regression,
it returns numerical values from ${\cal Y} = \mathbb{R}$.
\end{itemize}

With this API, a typical supervised learning task is therefore performed as follows:

\vskip0.3cm
\begin{pythoncode}
# Instantiate and set hyper-parameters
clf = DecisionTreeClassifier(max_depth=5)
# Learn a model from data
clf.fit(X_train, y_train)
# Make predictions on new data
y_hat = clf.predict(X_test)
\end{pythoncode}
\end{remark}


\section{Performance evaluation}

In the statistical sense, input and output variables $X_1, ..., X_p$ and $Y$
are \textit{random variables} taking jointly their values from ${\cal X}
\times {\cal Y}$ with respect to the joint probability distribution $P(X, Y)$,
where $X$ denotes the random vector $(X_1, ..., X_p)$. That is,
$P(X=\mathbf{x}, Y=y)$ is the probability of drawing the couple $(\mathbf{x},
y)$ at random from ${\cal X} \times {\cal Y}$.

Accordingly, learning  a model $\varphi$ whose predictions are as good as
possible can be defined as finding a model $\varphi$ which minimizes its
expected\footnote{$\mathbb{E}_X \{f(X)\}$ denotes the expected value of $f(x)$
(for $x \in {\cal X}$) with respect to the probability distribution of the
random variable $X$. It is defined as $$\mathbb{E}_X \{f(X)\} = \sum_{x \in {\cal
X}} P(X=x) f(x).$$} prediction error
\begin{equation}\label{eqn:generalization-error}
Err_{{\cal L}}(\varphi) = \mathbb{E}_{X, Y}\{ L(Y, \varphi(X)) | {\cal L} \},
\end{equation}
where ${\cal L}$ is the learning set used to build $\varphi$ and $L$ is a loss
function measuring the discrepancy between its two
arguments~\citep{geurts:2002}. Equation~\ref{eqn:generalization-error} is also
known as the \textit{generalization error} or \textit{test error} of the model $\varphi$ built from
${\cal L}$. It basically measures the prediction error of $\varphi$ over all
possible couples $(\mathbf{x}, y)\in {\cal X}\times{\cal Y}$, including the
observed couples from the learning set ${\cal L}$ but also all the
\textit{unseen} ones from ${\cal X}\times{\cal Y} \setminus {\cal L}$. Indeed,
the goal is not in fact to make the very-most accurate predictions over the
subset ${\cal L}$ of known data, but rather to learn a model which is correct
and reliable on all possible data.

For classification, the most common loss function is the \textit{zero-one} loss
function\footnote{$1(\text{\textit{condition}})$ denotes the unit function. It
is defined as
$$1(\text{\textit{condition}}) =
\begin{cases}
1 & \text{if \textit{condition} is true}\\
0 & \text{if \textit{condition} is false}
\end{cases}.
$$} $L(Y, \varphi(X)) = 1(Y \neq \varphi(X))$, where all
misclassifications are equally penalized. In this case, the generalization
error of $\varphi$ becomes the probability of misclassification of the model:
\begin{equation}
Err_{\cal L}(\varphi) = \mathbb{E}_{X, Y}\{ 1(Y \neq \varphi(X)) | {\cal L} \} = P(Y \neq \varphi(X)|{\cal L})
\end{equation}

Similarly, for regression, the most used loss function is the \textit{squared
error} loss $L(Y, \varphi(X)) = (Y - \varphi(X))^2$, where large differences
between the true values and the predicted values are penalized more heavily
than small ones. With this loss, the generalization error of the model becomes:
\begin{equation}
Err_{\cal L}(\varphi) = \mathbb{E}_{X, Y}\{ (Y - \varphi(X))^2 | {\cal L} \}
\end{equation}

\subsection{Estimating $Err_{\cal L}(\varphi)$}

In practice, the probability distribution $P(X, Y)$ is usually unknown, making
the direct evaluation of $Err_{\cal L}(\varphi)$ infeasible. Equivalently, it
is often not possible to draw additional data, thereby making infeasible the
empirical estimation of $Err_{\cal L}(\varphi)$ on a (virtually infinite) set
${\cal L}'$ drawn independently from ${\cal L}$. In most problems, ${\cal L}$
constitutes the only data available, on which both the model needs to be
learned and its generalization error estimated. As reviewed and described on
multiple occasions by several
authors~\citep{toussaint:1974,stone:1978,breiman:1984,kohavi:1995,nadeau:2003,hastie:2005,arlot:2010},
the generalization error in Equation~\ref{eqn:generalization-error} can however
be estimated in several ways.

The first and simplest estimate is the \textit{resubstitution estimate} or \textit{training error}. It
consists in empirically estimating $Err_{\cal L}(\varphi)$ on the same data as the
learning set ${\cal L}$ used to build $\varphi$, that is:
\begin{equation}\label{eqn:training-error}
\widehat{Err}_{\cal L}(\varphi) = \frac{1}{N} \sum_{(\mathbf{x_i}, y_i) \in {\cal L}} L(y_i, \varphi(\mathbf{x_i}))
\end{equation}
where $N$ is the size of ${\cal L}$.
In general, the resubstitution error is a poor estimate of
$Err_{\cal L}(\varphi)$. In particular, since most machine learning algorithms
aim at precisely minimizing Equation~\ref{eqn:training-error} (either directly
or indirectly), it typically results in an underestimation of the
generalization error -- which accounts for all couples $(\mathbf{x}, y)$, i.e., not only
those from ${\cal L}$.

The second approach is the \textit{test sample estimate}. It consists in
dividing the learning set ${\cal L}$ into two disjoint sets ${\cal
L}_\text{train}$ and ${\cal L}_\text{test}$, called
\textit{training set} and \textit{test set}, and then to use each part
respectively for learning the model and estimating its generalization error.
The test sample estimate over ${\cal L}_\text{test}$ of the model $\varphi$
built on ${\cal L}_\text{train}$ is therefore given by:
\begin{equation}\label{eqn:test-error}
\widehat{Err}_{{\cal L}_\text{train}}^{{\cal L}_\text{test}}(\varphi) = \frac{1}{N_\text{test}} \sum_{(\mathbf{x_i}, y_i) \in {\cal L}_\text{test}} L(y_i, \varphi(\mathbf{x_i}))
\end{equation}
where $N_\text{test}$ is the size of ${\cal L}_\text{test}$. As a rule-of-thumb,
${\cal L}_\text{train}$ is usually taken as $70\%$ of the samples in
${\cal L}$ and ${\cal L}_\text{test}$ as the remaining $30\%$, though
theoretical work~\citep{guyon:1997} suggests to progressively reduce the size of test set as
the size of ${\cal L}$ increases. In any case, care must taken when splitting ${\cal
L}$ into two subsets, so that samples from ${\cal L}_\text{train}$ can be
considered independent from those in ${\cal L}_\text{test}$ and drawn from the
same distribution. This is however usually guaranteed by drawing ${\cal
L}_\text{train}$ and ${\cal L}_\text{test}$ simply at random from ${\cal L}$.
While being an unbiased estimate of $Err_{\cal L}(\varphi)$, the test sample
estimate has the drawback that it reduces the
effective sample size on which the model is learned. If ${\cal L}$ is large,
then this is usually not an issue, but if ${\cal L}$ only contains a few dozens
of samples, then this strategy might impair the performance of the model.

When ${\cal L}$ is small, the \textit{$K$-fold cross-validation estimate} is
usually preferred over the test sample estimate. It consists in randomly
dividing the learning set ${\cal L}$ into $K$ disjoint subsets, denoted  ${\cal
L}_1, ..., {\cal L}_K$, and then to estimate the generalization error $Err_{\cal
L}(\varphi)$ as the average test error over the folds ${\cal L}_k$ of the
models $\varphi_k$  learned on the remaining data ${\cal L}\setminus{\cal
L}_k$:
\begin{equation}\label{eqn:cv-error}
\widehat{Err}_{\cal L}^{CV}(\varphi) = \frac{1}{K} \sum_{k=1}^K \widehat{Err}_{{\cal L}\setminus{\cal L}_k}^{{\cal L}_k}(\varphi_k)
\end{equation}
The assumption behind this approach is that since each model $\varphi_k$ is
built using almost all ${\cal L}$, they should all be close to the model
$\varphi$ learned on the entire set ${\cal L}$. As a result the unbiased estimates
$\smash{\widehat{Err}_{{\cal L}\setminus{\cal L}_k}^{{\cal L}_k}(\varphi_k)}$
should also all be close to $Err_{\cal L}(\varphi)$. While more computationally
intensive, the $K$-fold cross-validation estimate has the advantage that every
couple $(\mathbf{x}, y) \in {\cal L}$ is used both for learning a final model
$\varphi$ and estimating its generalization error. In a typical setting,
$K=10$, usually yielding stable and reliable estimates~\citep{kohavi:1995}.

\begin{remark}{Expected generalization error}
As we have shown, our goal is to estimate the generalization error $Err_{\cal
L}(\varphi)$ conditional on the learning set ${\cal L}$. A related quantity
is the \textit{expected} generalization error
\begin{equation}\label{eqn:expected-generalization-error}
Err = \mathbb{E}_{\cal L} \{ Err_{\cal L}(\varphi) \},
\end{equation}
averaging over everything which is random, including the randomness in the
learning set ${\cal L}$ used to produce $\varphi$. As discussed by
\citet{hastie:2005}, this quantity is close, yet different from $Err_{\cal
L}(\varphi)$. In particular, the authors point out that most estimates, including $K$-fold
cross-validation, effectively estimate
Equation~\ref{eqn:expected-generalization-error} rather than
Equation~\ref{eqn:generalization-error}.
\end{remark}

\subsection{Bayes model and residual error}

When the probability distribution $P(X, Y)$ is known, the model $\varphi_B$
which minimizes the generalization error of Equation~\ref{eqn:generalization-error}
can be derived analytically (and independently of any learning set
${\cal L}$).

By conditioning on $X$, the generalization error of this model becomes:
\begin{equation}\label{eqn:generalization-error-conditionned}
\mathbb{E}_{X, Y}\{ L(Y, \varphi_B(X)) \} = \mathbb{E}_{X}\{ \mathbb{E}_{Y|X} \{ L(Y, \varphi_B(X)) \} \}
\end{equation}
In this latter form, the model which minimizes
Equation~\ref{eqn:generalization-error-conditionned} is a model which
minimizes the inner expectation pointwise, that is:
\begin{equation}
\varphi_B(\mathbf{x}) = \argmin_{y \in {\cal Y}} \mathbb{E}_{Y|X=\mathbf{x}} \{ L(Y, y) \}
\end{equation}
In the literature, $\varphi_B$ is known as the \textit{Bayes model} and its
generalization error $Err(\varphi_B)$ as the \textit{residual error}. It represents the minimal
error that any supervised learning algorithm can possibly attain, that is
the irreducible error purely due to random deviations in the data.

In classification, when $L$ is the zero-one loss, the Bayes model is:
\begin{align}
\varphi_B(\mathbf{x}) &= \argmin_{y \in {\cal Y}} \mathbb{E}_{Y|X=\mathbf{x}} \{ 1(Y, y) \}\\
                      &= \argmin_{y \in {\cal Y}} P(Y \neq y|X=\mathbf{x})\\
                      &= \argmax_{y \in {\cal Y}} P(Y = y|X=\mathbf{x})
\end{align}
Put otherwise, the best possible classifier
consists in systematically predicting the most likely class $y \in \{1, ..., J\}$
given $X=\mathbf{x}$.

Similarly, for regression with the square error loss, we have:
\begin{align}
\varphi_B(\mathbf{x}) &= \argmin_{y \in {\cal Y}} \mathbb{E}_{Y|X=\mathbf{x}} \{ (Y - y)^2 \} \\
                      &= \mathbb{E}_{Y|X=\mathbf{x}} \{ Y \}
\end{align}
In other words, the best possible regressor consists in systematically predicting
the expected value of $Y$ given $X=\mathbf{x}$.

For practical problems, $P(X, Y)$ is unknown and the Bayes model cannot be
derived analytically. On simulated data however, where the distribution is
known, knowing $\varphi_B$ and comparing $Err(\varphi_B)$ with the test set
error of a model $\varphi$ can be beneficial for evaluating how effective
$\varphi$ is.


\section{Model selection}


\section{Classes of estimators}
    % See Pierre thesis
    % { API = fit, predict }
    % - Describe popular algorithms (trees, knn, linear models)
