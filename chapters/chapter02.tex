\chapter{Background}\label{ch:background}

\section{Learning from data}

In the examples introduced in Chapter~\ref{ch:introduction}, the objective
which is sought is to find a systematic way of predicting a phenomenon given a
set of measurements. In machine learning terms, this goal is formulated as the
{\it supervised learning} task of infering from collected data a model that
predicts the value of an output variable based on the observed values of input
variables. As such, finding an appropriate model is based on the assumption
that the output variable does not take its value at random and that there
exists a relation between the inputs and the output. In medicine for instance,
the goal is to find a decision rule (i.e., a model) from a set of past cases
(i.e., the collected data) for predicting the condition of an incoming patient
(i.e., the output value) given a set of measurements such as age, sex, blood
pressure or history (i.e., the input values).

To give a more precise formulation, let us arrange the set of measurements on a
case in a pre-assigned order, i.e., take the input values to be $x_1, x_2, ...,
x_p$, where $x_j \in {\cal X}_j$ (for $j = 1, ..., p$) corresponds to the value
of the input variable $X_j$. Together, the input values $(x_1, x_2, ..., x_p)$
form a $p$-dimensional input vector $\mathbf{x}$ taking its values in ${\cal
X}_1 \times ... \times {\cal X}_p = {\cal X}$, where ${\cal X}$ is defined as
the input space. Similarly, let us define as $y \in {\cal Y}$ the value of the
output variable $Y$, where ${\cal Y}$ is defined as the output space. By
definition, both the input and the output spaces are assumed to respectively
contain all possible input vectors and all possible output values. Let
us also note that input variables are sometimes known as {\it features}, input
vectors as {\it objects}, {\it examples} or {\it samples} and the output
variable as {\it target}.

Among variables that define the problem, we distinguish between two general
types. The former correspond to quantitative variables whose values are integer
or real numbers, such as age or blood pressure. The latter correspond to
qualitative variables whose values are symbolic, such as gender or condition.
Formally, we define them as follows:

\begin{definition}
A variable $X_j$ is \emph{numerical} or \emph{ordered} if ${\cal X}_j$ is a
totally ordered set.
\end{definition}

\begin{definition}
A variable $X_j$ is \emph{categorical} if ${\cal X}_j$ is a finite set of values,
without any natural order.
\end{definition}

In a typical supervised learning task, past observations are summarized by a
dataset called {\it learning set}. It consists in a set of observed input
vectors together with their actual output value and formally defined as
follows:

\begin{definition}
A \emph{learning set} ${\cal L}$ is a set of $N$
pairs of input vectors and output values $(\mathbf{x}_1, y_1), ...,
(\mathbf{x}_N, y_N)$, where $\mathbf{x}_i \in {\cal X}$ and $y_i \in {\cal Y}$.
\end{definition}

Equivalently, a set of $p$-input vectors $\mathbf{x}_i$ (for $i=1, ..., N$) can
be denoted by a $N\times p$ matrix $\mathbf{X}$, whose rows $i=1, ..., N$
correspond to input vectors $\mathbf{x}_i$ and columns $j=1, ..., p$ to input
variables $X_j$. Similarly, the corresponding output values can be written as a
vector $\mathbf{y}=(y_1, ..., y_N)$.

\begin{remark}{Data representation}
For optimal implementations of machine learning algorithms, data needs to
represented using structures which allows for high-performance numerical
computation. In this work, code snippets will be described under  that assumption
that data is represented using a data structure similar to a NumPy
array~\citep{vanderwalt:2011}.

A NumPy array is basically a multidimensional uniform collection of values, all
of the same type and organized in a given shape. For instance, a matrix
$\mathbf{X}$ can be represented as a 2-dimensional NumPy array of shape
$N \times p$ that contains numbers (e.g., floating point values or integers).
This structure allows for random access in constant time, vectorized operations
and efficient memory usage.

Additionally, using a data representation which is close to the matrix
formulation makes it possible to write implementations that are close to their
original textbook formulation, thereby making them easier to understand and
maintain.
\end{remark}

In this framework, the supervised learning task can be stated as learning a
function $\varphi: {\cal X} \mapsto {\cal Y}$ from a learning set ${\cal
L}=(\mathbf{X}, \mathbf{y})$. In particular, the goal of the learning task is
to find a model such that its predictions $\varphi(\mathbf{x})$, also denoted
by the variable $\hat{Y}$, are as good as possible. If $Y$ is a categorical
variable then the learning task is a classification problem. If $Y$ is
numerical variable, then learning task is a regression problem. Without loss of
generality, the resulting models, or \textit{estimators}, can be defined as
follows:

\begin{definition}
A \emph{classifier} or \emph{classification rule} is a function $\varphi: {\cal X}
\mapsto {\cal Y}$, where ${\cal Y}$ is a finite set of classes denoted $\{1, 2, ..., J\}$.
\end{definition}

\begin{definition}
A \emph{regressor} is a function $\varphi: {\cal X} \mapsto {\cal Y}$, where ${\cal Y}=\mathbb{R}$.
\end{definition}

\begin{remark}{Estimator interface}
\todo{}
\end{remark}


    % { API = array }
    % - Data => variables (types), notation, etc
    % - ML tasks, focus on supervised learning
    %     - Classification regression




\section{Estimating performance}
    % See Pierre thesis
    % { API = score }
    % - train/test accuracy => underfitting/overfitting
    % - cv accuracy
    % - metrics
    % - bayes error => jusqu'o√π?


\section{Classes of estimators}
    % See Pierre thesis
    % { API = fit, predict }
    % - Describe popular algorithms (trees, knn, linear models)
