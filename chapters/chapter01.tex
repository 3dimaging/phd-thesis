\chapter{Introduction}\label{ch:introduction}

In various fields of science, technology and humanities, as in medicine,
meteorology, finance or robotics to cite a few, experts aim at predicting a
phenomenon based on past observations or measurements. For instance,
meteorologists try to forecast the weather for the next days from the climatic
conditions of the previous days. In medicine, practicians collect measurements
and information such as blood pressure, age or history for diagnosing the
condition of incoming patients. Similarly, in chemistry, compounds are analyzed
using mass spectrometry measurements in order to determine whether they contain
a given type of molecules or atoms. In all of these cases, the goal is
the prediction of a response variable based on a set of observed predictor
variables.

For centuries, scientists have addressed these questions by deriving
theoretical frameworks from first principles or have accumulated knowledge in
order to model, analyze and understand the pheno\-menon under study. For
example, practicians know from past experience that elderly heart attack
patients with low blood pressure are generally high risk. Similarly,
meteorologists know from elementary climate models that one hot, high pollution
day is likely to be followed by another. For an increasing number of problems
however, standard approaches start showing their limits. Identifying the
genetic risk factors for heart disease for example, where knowledge is still
very sparse, is nearly impractical for the cognitive abilities of humans given
the high complexity and intricacy of interactions that exist between genes.
Likewise, for very fine-grained meteorological forecasts, a large number of
variables need to be taken into account, which quickly goes beyond the
capabilities of experts to put them all into a unified set equations. To break
this cognitive barrier and further advance science, machines of increasing
speed and capacity have been built and designed since the mid-twentieth century
to assist humans in their calculations. However, alongside this progress in
terms of hardware, developments in theoretical computer science, artificial
intelligence and statistics have made machines to become more than calculators.
Recent advances have made them experts of their own kind, capable to learn from
data and to uncover by themselves the predictive structure of problems.
Techniques and algorithms that have stemmed from the field of {\it machine
learning} have now become a powerful tool for the analysis of complex and large
data, successfully assisting scientists in numerous breakthroughs of various
fields of science and technology.

Formally, machine learning can be defined as the study of systems that can
learn from data without being explicitely programmed. According to
\citet{mitchell:1997}, a computer program is said to learn from data, with
respect to some class of tasks and performance measure if its performance at
thoses tasks improves with data. In particular, machine learning provides
algorithms that are able to solve classification or regression tasks, hence
bringing now automated procedures for the prediction of a phenomenon based on
past observations. However, the goal of machine learning is not only to produce
algorithms making accurate predictions, it is also to provide insights on the
predictive structure of the data~\citep{breiman:1984}. If we are aiming at the
latter, then our goal is to understand what variables or interactions of
variables drive the phenomenon. For practitioners, which are not experts in
machine learning, interpretability is indeed often as important as prediction
accuracy. It allows for a better understanding of the phenomenon under study, a
finer exploration of the data and an easier self-appropriation of the results.
By contrast, when an algorithm is used as a black box, yielding results
seemingly out of nowhere, it may indeed be difficult to thrust or accept if it
cannot be understood how and why the procedure came to them. Unfortunately, the
current state-of-the-art in machine learning often makes it difficult for
non-experts to understand and interpret the results of an algorithm. While
considerable efforts have been put to improve their prediction accuracy, it is
still not clearly understood what makes machine learning algorithms truly work,
and under what assumptions. Likewise, few of them actually provide clear and
insightful explanations about the results they generate.

In this context, the goal of this thesis is to provide a comprehensive analysis
of a class of algorithms known as forests of decision
trees~\citep{breiman:2001,geurts:2006}. While these methods have proven to be
a robust, accurate and successful tool for solving various machine learning
tasks, including classification, regression, density estimation, manifold
learning or semi-supervised learning~\citep{criminisi:2011}, there remains
many gray areas in its theoretical understanding, its practical implementation
and its interpretation. \todo{In the first part... }

% foreword on implementation

% Goals =
% - a comprehensive study of decision trees and random forests
%    from a theoretical and theoretical point of view
% - of their implementation (how), which is easily overlooked
% - insights on interpretability (why)


% ==> In plain English (no maths!)
% General introduction to ML
%  => pourquoi se passer de l'humain et désirer qu'une machine apprenne? (Zighed pg 20)
% Goals and purposes of ML, examples of applications
% => interprétabilité, extraction de connaissances à partir de données
% Tree-based methods (introduction, atouts, etc)
% Content of the thesis

% Reuse content from tentative paper
