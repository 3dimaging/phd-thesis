\chapter{Introduction}\label{ch:introduction}

In various fields of science and technology, like in medicine, psychology,
meteorology, finance or robotics to cite a few, experts aim at predicting a
phenomenon based on past observations or measurements. For instance,
meteorologists try to forecast the weather for the next days from the climatic
conditions of the previous days. In medicine, practicians collect measurements
and information such as blood pressure, age or history for diagnosing the
condition of incoming patients. Similarly, in chemistry, compounds are analyzed
using mass spectrometry measurements in order to determine whether they contain
a given type of molecules or atoms. In all of these cases, the goal is
the prediction of a response variable based on a set of predictor
variables.

For centuries, scientists have addressed these questions by deriving
theoretical frameworks from first principles or have accumulated knowledge in
order to model, analyze and understand the pheno\-menon under study. For
example, practicians know from past experience that elderly heart attack
patients with low blood pressure are generally high risk. Similarly,
meteorologists know from elementary climate models that one hot, high pollution
day is likely to be followed by another. For an increasing number of problems
however, standard approaches start showing their limits. Identifying the
genetic risk factors for heart disease for example, where knowledge is still
very sparse, is nearly impractical for the cognitive abilities of humans given
the high complexity and intricacy of interactions that exist between genes.
Likewise, for very fine-grained meteorological forecasts, a large number of
variables need to be taken into account, which quickly goes beyond the
capabilities of experts to put them all into a unified set equations. To break
this cognitive barrier and further advance science, machines of increasing
speed and capacity have been built and designed since the mid-twentieth century
to assist humans in their calculations. However, alongside this progress in
terms of hardware, developments in theoretical computer science, artificial
intelligence and statistics have made machines to become more than calculators.
Recent advances have made them experts of their own kind, capable to learn from
data and to uncover by themselves the predictive structure of problems.
Techniques and algorithms that have stemmed from the field of {\it machine
learning} have now become a powerful tool for the analysis of complex and large
data, successfully assisting scientists in numerous breakthroughs of various
fields of science and technology.

Formally, machine learning can be defined as the study of systems that can
learn from data without being explicitely programmed. According to
\citet{mitchell:1997}, a computer program is said to learn from data, with
respect to some class of task and performance measure if its performance at
thoses tasks improves with data. In particular, machine learning provide
algorithms that are able to solve classification or regression tasks, hence
yielding now automated procedures for the prediction of a phenomenon based on
past observations. However, the goal of machine learning is not only to produce
algorithms making accurate predictions, it is also to provide insight and
undestanding into the predictive structure of the data~\citep{breiman:1984}.



% ==> In plain English (no maths!)
% General introduction to ML
%  => pourquoi se passer de l'humain et désirer qu'une machine apprenne? (Zighed pg 20)
% Goals and purposes of ML, examples of applications
% => interprétabilité, extraction de connaissances à partir de données
% Tree-based methods (introduction, atouts, etc)
% Content of the thesis

% Reuse content from tentative paper


