Table of content
================

Ideas:
    - Literate programming
    - Analyse de complexité au fur et à mesure
    - Comparaison des implémentations

______________________________________________________________________________


                                    ~~~

                        UNDERSTANDING RANDOM FORESTS

                                    ~~~


1. Introduction
---------------

* General introduction to ML
* Goals and purposes of ML, examples of applications
    - Accuracy
    - Interpretability (CART 1.3)
* Tree-based methods (introduction, atouts, etc)
* Content of the thesis


Part I: Growing random forests
==============================

2. Background                                                             12/01
-------------

* Learning from data
    { API = array }
    - Data => variables (types), notation, etc
    - ML tasks, focus on supervised learning
        - Classification regression
    - estimator { API = fit, predict }
* Estimating performance
    { API = score }
    - train/test accuracy => underfitting/overfitting
    - cv accuracy
    - metrics
* Classes of estimators
    - Describe popular algorithms (trees, knn, linear models)


3. Classification and regression trees                                    02/02
--------------------------------------

* Citer les premiers travaux
* Tree structured estimators
    > (Titanic classification problem)
    > Estimators as partitions
        CART page 4
    > Data structures (arrays vs. object-oriented structures)
    > (Mention multi-output)
* Growing decision trees
    > Recursive partition
        [ max_depth, max_leaf_nodes, min_samples_split, min_samples_leaf ]
        - Critères d'arrêt
        - Depth/Breadth/Best first
    > Splitting nodes
        [ max_features, min_samples_leaf ]
        - Best splits
            + Greedy approximation of optimal trees
            + How to evaluate multiple candidate thresholds cheaply for
              a given feature (algorithm, f-ordered)
            + Sorting algorithms
        - Approximately best splits
            + Binning
            + Subsampling
        - Weak learners (different from axis-aligned splits)
    > Impurity criteria
        - Efficient implementation for iterative evaluation
        - Effet du critère sur les cuts
            End-cut preference
            Normalisation par l'entropy (cf. Vincent, Louis)
            Effet sur la structure des arbres générés
        - Sample weighting
            Negative weights? (@ndawe)
* Predicting
    > Procedure
    > Consistency


4. Forests of decision trees                                              16/02
----------------------------

* Bias-variance tradeoff
* Randomization
    > Subsampling (with/without replacement)
    > Splitting nodes
        - Best splits (among max_features)
        - Random splits (random threshold, random index)
* Shared computations
    > Splitters
        - (Independent splitters)
        - Pre-sorting + data reorganization (Breiman's strategy)
        - Pre-sorting + sample_mask
    > Multi-threading


5. Complexity analysis                                                    02/03
----------------------

* Overall complexity
    - Best cases => Master theorem and/or Akra Bazzi method
        RF: O(n log^2 n)
        ETs: O(n log n)
    - Worst cases
        RF: O(log(H(n)))
        ETs: O(n^2)
    - Average case => Should still be good (see Quicksort average complexity)
      http://en.wikipedia.org/wiki/Quicksort
      Solve: T(n) = nlog(n) + 1/(n-1) \sum_{i=1}^{n-1} [T(i) + T(n-i)]
* Comparison of existing implementations
    - Experiments and/or implementation details


Part II: Interpreting random forests
====================================

(introduction)

6. Variables importances (NIPS)                                           23/03
------------------------

* Motivation
* Variable importances
    - as implemented
    - as derived from totally randomized trees
* Importances of relevant and irrelevant variables
* Generalization to other impurity measures
* Pruning and random subspaces
* Non-totally randomized trees (max_features > 1)
* Binary trees
* Illustration
    - Paper
    - Expériences de Olivier?


7. Applications on real data                                              13/04
----------------------------

TODO
(Vincent, Cambridge)


Part III: Subsampling data
==========================

8. Random patches (ECML paper)                                            27/04
-----------------

* Motivation
    > Bias-variance discussion
    > Big(-ish) data
* Accuracy
* Memory usage


~~~


A. References
-------------


B. Notations
------------
