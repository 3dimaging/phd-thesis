Table of content
================

Ideas:
    - Literate programming
    - Analyse de complexité au fur et à mesure
    - Comparaison des implémentations


Part I: Growing trees
=====================

1. Background
-------------

* (Introduction)
* Machine learning
    > Data
    > Estimators
        - Classifiers
        - Regressors
    > API
* Purposes


2. Classification and regression trees
--------------------------------------

* Tree structured estimators
    > (Titanic classification problem)
    > Estimators as partitions
    > Data structures (arrays vs. object-oriented structures)
    > Mention multi-output
* Growing decision trees
    > Recursive partition
        [ max_depth, max_leaf_nodes, min_samples_split, min_samples_leaf ]
        - Depth first
        - Breadth first
        - Best first
    > Splitting nodes
        [ max_features, min_samples_leaf ]
        - Best splits
            + Greedy approximation of optimal trees
            + How to evaluate multiple candidate thresholds cheaply for a given feature
            + Sorting algorithms (NB: arrays to be sorted are often degenerated)
        - Weak learners (different from axis-aligned splits)
    > Criteria
        - Efficient implementation for iterative evaluation
        - Effet du critère sur les cuts
        - Sample weighting
* Predicting


3. Forests of decision trees
----------------------------

* Bias-variance tradeoff
* Randomization
    > Subsampling (with/without replacement)
    > Splitting nodes
        - Best splits (amonng max_features)
        - Approximately best splits
            + Binning
            + Subsampling
        - Random splits
* Shared computations
    > Splitters
        - Independent splitters
        - Pre-sorting + data reorganization (Breiman's strategy)
        - Pre-sorting + sample_mask
    > Multi-threading


4. Complexity analysis
----------------------

* Best cases => Master theorem and/or Akra Bazzi method
    RF: O(n log^2 n)
    ETs: O(n log n)
* Worst cases
    RF: O(log(H(n)))
    ETs: O(n^2)
* Average case => Should still be good (see Quicksort average complexity)
  http://en.wikipedia.org/wiki/Quicksort#Average-case_analysis_using_recurrences
  Solve: T(n) = nlog(n) + 1/(n-1) \sum_{i=1}^{n-1} [T(i) + T(n-i)]

* Comparison of existing implementations



Part II: Subsampling data
=========================

5. Random patches (ECML paper)
-----------------

* Bias-variance discussion
* Memory constraints / big-ish data



Part III: Interpreting
======================

6. Variables importances (NIPS)
------------------------

todo


7. Applications on real data (Vincent, Cambridge)
-------------------------------------------------

todo



~~~


8. References
-------------


9. Notations
------------
